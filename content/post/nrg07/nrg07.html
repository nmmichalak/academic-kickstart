---
title: "Testing Within-Subjects Contrasts (Repeated Measures) in R"
output: 
  html_document: 
    theme: readable
---



<div id="within-subjects-design" class="section level1">
<h1>Within-Subjects Design</h1>
<blockquote>
<p>In a within-subjects design, subjects give responses across multiple conditions or across time. In other words, measures are repeated across levels of some condition or across time points. For example, subjects can report how happy they feel when they see a sequence of positive pictures and another sequence of negative pictures. In this case, we’d observe each subjects’ happiness in both positive and negative conditions. As another example, we could measure subjects’ job satisifcation every month for 3 months. Here we’d observe job satisfaction for each subject across time. In both cases, subject scores across conditions or time very likely correlate with eachother – they are dependent.</p>
</blockquote>
</div>
<div id="what-are-contrasts" class="section level1">
<h1>What are contrasts?</h1>
<blockquote>
<p>Broadly, contrasts test focused research hypotheses. A contrast comprises a set of weights or numeric values that represent some comparison. For example, when comparing two experimental group means (i.e., control vs. treatment), you can apply weights to each group mean and then sum them up. This is the same thing as subtracting one group’s mean from the other’s.</p>
</blockquote>
<div id="heres-a-quick-demonstration" class="section level2">
<h2>Here’s a quick demonstration</h2>
<pre class="r"><code># group means
control &lt;- 5
treatment &lt;- 3

# apply contrast weights and sum up the results
sum(c(control, treatment) * c(1, -1))</code></pre>
<pre><code>## [1] 2</code></pre>
</div>
</div>
<div id="model-and-conceptual-assumptions-for-linear-regression" class="section level1">
<h1>Model and Conceptual Assumptions for Linear Regression</h1>
<blockquote>
<ul>
<li><strong>Correct functional form.</strong> Your model variables share linear relationships.<br />
</li>
<li><strong>No omitted influences.</strong> This one is hard: Your model accounts for all relevant influences on the variables included. All models are wrong, but how wrong is yours?<br />
</li>
<li><strong>Accurate measurement.</strong> Your measurements are valid and reliable. Note that unreliable measures can’t be valid, and reliable measures don’t necessairly measure just one construct or even your construct.<br />
</li>
<li><strong>Well-behaved residuals.</strong> Residuals (i.e., prediction errors) aren’t correlated with predictor variables or eachother, and residuals have constant variance across values of your predictor variables.</li>
</ul>
</blockquote>
</div>
<div id="model-and-conceptual-assumptions-for-repeated-measures-anova" class="section level1">
<h1>Model and Conceptual Assumptions for Repeated Measures ANOVA</h1>
<blockquote>
<ul>
<li><strong>All change scores variances are equal.</strong> Similar to the homogenous group variance assumption in between-subjects ANOVA designs, within-subjects designs require that all change score variances (e.g., subject changes between time points, within-subject differences between conditions) are equal. This means that if you compute the within-subject differences between all pairiwse levels (e.g., timepoints, treatment levels), the variances of those parwises differences must all be equal. For example, if you ask people how satisifed they are with their current job every month for 3 months, then the variance of month 2 - month 1 should equal the variance of month 3 - month 2 and the variance of month 3 - month 1. As you might be thinking, this assumption is very strict and probably not realistic in many cases.</li>
</ul>
</blockquote>
</div>
<div id="a-different-take-on-the-homogenous-change-score-variance-assumption" class="section level1">
<h1>A different take on the homogenous change score variance assumption</h1>
<blockquote>
<ul>
<li><strong>Sphericity and a special case, compound symmetry.</strong> Sphericity is the matrix algebra equivalent to the homogenous change score variance assumption. Compound symmtry is a special case of sphericity. A variance-covariance matrix that satisifies compound symmetry has equal variances (the values in the diagonal) and equal covariances (the values above or below the diagonal).<br />
<strong>Short explanation</strong>: Sphericity = homogenous change score variance = compound symmetry</li>
</ul>
</blockquote>
<div id="print-an-example-of-a-matrix-that-satisfies-compound-symmetry" class="section level2">
<h2>Print an example of a matrix that satisfies compound symmetry</h2>
<pre class="r"><code># 3 x 3 matrix, all values = 0.20
covmat &lt;- matrix(0.20, nrow = 3, ncol = 3)

# for kicks, make all the variances = 0.50
diag(covmat) &lt;- rep(0.50, 3)

# print
covmat</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.5  0.2  0.2
## [2,]  0.2  0.5  0.2
## [3,]  0.2  0.2  0.5</code></pre>
</div>
<div id="libraries" class="section level2">
<h2>Libraries</h2>
<pre class="r"><code>library(tidyverse)
library(knitr)
library(AMCP)
library(MASS)
library(afex)
library(lsmeans)

# select from dplyr
select &lt;- dplyr::select
recode &lt;- dplyr::recode</code></pre>
</div>
<div id="load-data" class="section level2">
<h2>Load data</h2>
<blockquote>
<p>From Chapter 11, Table 5 in Maxwell, Delaney, &amp; Kelley (2018)<br />
From <code>help(&quot;C11T5&quot;)</code>: “The data show that 12 participants have been observed in each of 4 conditions. To make the example easier to discuss, let’s suppose that the 12 subjects are children who have been observed at 30, 36, 42, and 48 months of age. Essentially, for the present data set, 12 children were each observed four times over an 18 month period. The dependent variable is the age-normed general cognitive score on the McCarthy Scales of Children’s Abilities. Interest is to determine if the children were sampled from a population where growth in cognitive ability is more rapid or less rapid than average.”</p>
</blockquote>
<pre class="r"><code>data(&quot;C11T5&quot;)
C11T5$id &lt;- factor(1:12)

# print entire dataset
C11T5 %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Months30</th>
<th align="right">Months36</th>
<th align="right">Months42</th>
<th align="right">Months48</th>
<th align="left">id</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">108</td>
<td align="right">96</td>
<td align="right">110</td>
<td align="right">122</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="right">103</td>
<td align="right">117</td>
<td align="right">127</td>
<td align="right">133</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="right">96</td>
<td align="right">107</td>
<td align="right">106</td>
<td align="right">107</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="right">84</td>
<td align="right">85</td>
<td align="right">92</td>
<td align="right">99</td>
<td align="left">4</td>
</tr>
<tr class="odd">
<td align="right">118</td>
<td align="right">125</td>
<td align="right">125</td>
<td align="right">116</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="right">110</td>
<td align="right">107</td>
<td align="right">96</td>
<td align="right">91</td>
<td align="left">6</td>
</tr>
<tr class="odd">
<td align="right">129</td>
<td align="right">128</td>
<td align="right">123</td>
<td align="right">128</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="right">90</td>
<td align="right">84</td>
<td align="right">101</td>
<td align="right">113</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="right">84</td>
<td align="right">104</td>
<td align="right">100</td>
<td align="right">88</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="right">96</td>
<td align="right">100</td>
<td align="right">103</td>
<td align="right">105</td>
<td align="left">10</td>
</tr>
<tr class="odd">
<td align="right">105</td>
<td align="right">114</td>
<td align="right">105</td>
<td align="right">112</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="right">113</td>
<td align="right">117</td>
<td align="right">132</td>
<td align="right">130</td>
<td align="left">12</td>
</tr>
</tbody>
</table>
</div>
<div id="restructure-data" class="section level2">
<h2>Restructure data</h2>
<pre class="r"><code>lC11T5 &lt;- C11T5 %&gt;%
  gather(key = month, value = score, -id) %&gt;% 
  mutate(monthnum = month %&gt;% recode(&quot;Months30&quot; = 0, &quot;Months36&quot; = 6, &quot;Months42&quot; = 12, &quot;Months48&quot; = 18),
         linear = monthnum %&gt;% recode(`0` = -3, `6` = -1, `12` = 1, `18` = 3),
         quadratic = monthnum %&gt;% recode(`0` = 1, `6` = -1, `12` = -1, `18` = 1),
         cubic = monthnum %&gt;% recode(`0` = -1, `6` = 3, `12` = -3, `18` = 1),
         monthfac = month %&gt;% factor(ordered = TRUE))

# print first and last 5 observations from dataset
head(lC11T5) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">id</th>
<th align="left">month</th>
<th align="right">score</th>
<th align="right">monthnum</th>
<th align="right">linear</th>
<th align="right">quadratic</th>
<th align="right">cubic</th>
<th align="left">monthfac</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Months30</td>
<td align="right">108</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Months30</td>
<td align="right">103</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Months30</td>
<td align="right">96</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Months30</td>
<td align="right">84</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">Months30</td>
<td align="right">118</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">Months30</td>
<td align="right">110</td>
<td align="right">0</td>
<td align="right">-3</td>
<td align="right">1</td>
<td align="right">-1</td>
<td align="left">Months30</td>
</tr>
</tbody>
</table>
<pre class="r"><code>tail(lC11T5) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">id</th>
<th align="left">month</th>
<th align="right">score</th>
<th align="right">monthnum</th>
<th align="right">linear</th>
<th align="right">quadratic</th>
<th align="right">cubic</th>
<th align="left">monthfac</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>43</td>
<td align="left">7</td>
<td align="left">Months48</td>
<td align="right">128</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
<tr class="even">
<td>44</td>
<td align="left">8</td>
<td align="left">Months48</td>
<td align="right">113</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
<tr class="odd">
<td>45</td>
<td align="left">9</td>
<td align="left">Months48</td>
<td align="right">88</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
<tr class="even">
<td>46</td>
<td align="left">10</td>
<td align="left">Months48</td>
<td align="right">105</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
<tr class="odd">
<td>47</td>
<td align="left">11</td>
<td align="left">Months48</td>
<td align="right">112</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
<tr class="even">
<td>48</td>
<td align="left">12</td>
<td align="left">Months48</td>
<td align="right">130</td>
<td align="right">18</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Months48</td>
</tr>
</tbody>
</table>
</div>
<div id="visualize-relationships" class="section level2">
<h2>Visualize relationships</h2>
<blockquote>
<p>It’s always a good idea to look at your data. Check some assumptions.</p>
</blockquote>
</div>
<div id="children-scores-means-and-95-confidence-intervals-at-each-time-point" class="section level2">
<h2>Children scores + means and 95% confidence intervals at each time point</h2>
<pre class="r"><code>lC11T5 %&gt;% 
  ggplot(mapping = aes(x = month, y = score)) +
  geom_point(position = position_jitter(0.1)) +
  stat_summary(geom = &quot;point&quot;, fun.data = mean_cl_normal, color = &quot;red&quot;, size = 2) +
  stat_summary(geom = &quot;errorbar&quot;, fun.data = mean_cl_normal, color = &quot;red&quot;, width = 0) +
  theme_bw()</code></pre>
<p><img src="/post/nrg07/nrg07_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div id="qq-plots" class="section level3">
<h3>QQ-plots</h3>
<blockquote>
<p>Do observations look normal at each time point?</p>
</blockquote>
<pre class="r"><code>lC11T5 %&gt;% 
  ggplot(mapping = aes(sample = score)) +
  geom_qq() +
  facet_wrap(facets = ~ month, scales = &quot;free&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/nrg07/nrg07_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="variance-covariance-matrix-among-4-timepoints" class="section level2">
<h2>Variance-covariance matrix among 4 timepoints</h2>
<blockquote>
<p>Does this look like compound symmetry?</p>
</blockquote>
<pre class="r"><code>C11T5 %&gt;% select(-id) %&gt;% cov(use = &quot;pairwise.complete.obs&quot;) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Months30</th>
<th align="right">Months36</th>
<th align="right">Months42</th>
<th align="right">Months48</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Months30</td>
<td align="right">188.0000</td>
<td align="right">154.36364</td>
<td align="right">127.3636</td>
<td align="right">121.18182</td>
</tr>
<tr class="even">
<td>Months36</td>
<td align="right">154.3636</td>
<td align="right">200.54545</td>
<td align="right">143.6364</td>
<td align="right">97.45455</td>
</tr>
<tr class="odd">
<td>Months42</td>
<td align="right">127.3636</td>
<td align="right">143.63636</td>
<td align="right">178.0000</td>
<td align="right">168.09091</td>
</tr>
<tr class="even">
<td>Months48</td>
<td align="right">121.1818</td>
<td align="right">97.45455</td>
<td align="right">168.0909</td>
<td align="right">218.00000</td>
</tr>
</tbody>
</table>
</div>
<div id="test-polynomial-contrasts-with-pooled-error-term" class="section level2">
<h2>Test polynomial contrasts with <strong>pooled error term</strong></h2>
<div id="linear-regression-method" class="section level3">
<h3>Linear regression method</h3>
<blockquote>
<p>Below I ID use a blocking factor. This could seem nuts, but this approach “controls” for (i.e., partials out variance due to) subject ID. The polynomial contrast tests are equivalent to those you’d see from a special function or command for repeated measures ANOVA. I’ll demonstrate that later in this post.</p>
</blockquote>
<pre class="r"><code>lm01 &lt;- lm(score ~ linear + quadratic + cubic + id, data = lC11T5)</code></pre>
</div>
<div id="anova-results" class="section level3">
<h3>ANOVA results</h3>
<pre class="r"><code>anova(lm01)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: score
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## linear     1    540  540.00  8.8833  0.005369 ** 
## quadratic  1     12   12.00  0.1974  0.659722    
## cubic      1      0    0.00  0.0000  1.000000    
## id        11   6624  602.18  9.9063 1.314e-07 ***
## Residuals 33   2006   60.79                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="interpretation" class="section level3">
<h3>Interpretation</h3>
<blockquote>
<p>We found evidence that children’s age-normed general cognitive score increased linearly over the course of 18 months, <em>F</em>(1, 33) = 8.88, <em>p</em> = .005. However, we found no sufficient evidence for a quadratic trend, <em>F</em>(1, 33) = 0.20, <em>p</em> = .660, nor a cubic trend, <em>F</em>(1, 33) &lt; .001, p &gt; .99.</p>
</blockquote>
</div>
<div id="get-results-via-algebraic-formulas" class="section level3">
<h3>Get results via algebraic formulas</h3>
<blockquote>
<p>Below, I test the same polynomial contrasts using algebraic equations. There’s no need to follow every last piece of code or understand each formula. I’m presenting this just to be thorough. If you’re curious, these formulas are explained in Chapter 11 of Maxwell, Dalaney, and Kelly (2018).</p>
</blockquote>
<pre class="r"><code># contrasts
linear &lt;- c(-3, -1, 1, 3)
quadratic &lt;- c(1, -1, -1, 1)
cubic &lt;- c(-1, 3, -3, 1)

# estimate (mean difference)
linest &lt;- mean(as.matrix(C11T5[, -5]) %*% linear)
quadest &lt;- mean(as.matrix(C11T5[, -5]) %*% quadratic)
cubest &lt;- mean(as.matrix(C11T5[, -5]) %*% cubic)
est &lt;- c(linest, quadest, cubest)

# number of participants
n &lt;- nrow(C11T5)

# sum of squares for the contrast
linssc &lt;- n * linest^2 / sum(linear^2)
quadssc &lt;- n * quadest^2 / sum(quadratic^2)
cubssc &lt;- n * cubest^2 / sum(cubic^2)
ssc &lt;- c(linssc, quadssc, cubssc)

# mean squared error
mse &lt;- anova(lm(score ~ linear + quadratic + cubic + id, data = lC11T5))$`Mean Sq`[5]

# F-ratio
fratio &lt;- ssc / mse

# number of levels (i.e., timepoints)
nlvls &lt;- nlevels(lC11T5$monthfac)

# degrees of freedom
df1 &lt;- 1
df2 &lt;- (nlvls - 1) * (n - 1)

# p-value
p &lt;- 1 - pf(q = fratio, df1 = df1, df2 = df2)

# alpha
alpha &lt;- 0.05

# F critical
fcrit &lt;- qf(p = 1 - alpha, df1 = df1, df2 = df2)

# standard error
linse &lt;- sqrt(mse * sum(linear^2 / n))
quadse &lt;- sqrt(mse * sum(quadratic^2 / n))
cubse &lt;- sqrt(mse * sum(cubic^2 / n))
se &lt;- c(linse, quadse, cubse)

# lower and upper bound
lwr &lt;- c(linest, quadest, cubest) - sqrt(fcrit) * se
upr &lt;- c(linest, quadest, cubest) + sqrt(fcrit) * se

# table of results
tibble(est, se, fratio, p, lwr, upr)</code></pre>
<pre><code>## # A tibble: 3 x 6
##     est    se fratio       p    lwr   upr
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1    30 10.1   8.88  0.00537   9.52 50.5 
## 2    -2  4.50  0.197 0.660   -11.2   7.16
## 3     0 10.1   0     1       -20.5  20.5</code></pre>
</div>
<div id="get-the-same-results-using-afexaov_car" class="section level3">
<h3>Get the same results using <code>afex::aov_car</code></h3>
</div>
<div id="fit-model" class="section level3">
<h3>Fit model</h3>
<pre class="r"><code>aovcar1 &lt;- aov_car(score ~ monthfac + Error(id / monthfac), data = lC11T5)

# equivalent
# aovez1 &lt;- aov_ez(id = &quot;id&quot;, dv = &quot;score&quot;, data = lC11T5, within = &quot;monthfac&quot;)
# aovlmer1 &lt;- aov_4(score ~ monthfac + (1 + monthfac | id), data = lC11T5)</code></pre>
</div>
<div id="print-polynomial-contrasts" class="section level3">
<h3>Print polynomial contrasts</h3>
<pre class="r"><code>aovcar1 %&gt;% lsmeans(specs = ~ monthfac) %&gt;% contrast(method = &quot;poly&quot;)</code></pre>
<pre><code>##  contrast  estimate   SE df t.ratio p.value
##  linear          30 10.1 33  2.980  0.0054 
##  quadratic       -2  4.5 33 -0.444  0.6597 
##  cubic            0 10.1 33  0.000  1.0000</code></pre>
</div>
</div>
<div id="test-polynomial-contrasts-with-separate-error-term" class="section level2">
<h2>Test polynomial contrasts with <strong>separate error term</strong></h2>
<div id="long-way" class="section level3">
<h3>Long way</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Subset each column of data that represents a given timepoint<br />
</li>
<li>Multiply that entire column of data by the contrast weight<br />
</li>
<li>Add up the columns (makes one column of data)<br />
</li>
<li>Test whether the mean of those values = 0 (i.e., one-sample t-test)</li>
</ol>
</blockquote>
<pre class="r"><code>t.test(C11T5[, &quot;Months30&quot;] * -3 + C11T5[, &quot;Months36&quot;] * -1 + C11T5[, &quot;Months42&quot;] * 1 + C11T5[, &quot;Months48&quot;] * 3)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  C11T5[, &quot;Months30&quot;] * -3 + C11T5[, &quot;Months36&quot;] * -1 + C11T5[,     &quot;Months42&quot;] * 1 + C11T5[, &quot;Months48&quot;] * 3
## t = 2.2414, df = 11, p-value = 0.04659
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   0.5403653 59.4596347
## sample estimates:
## mean of x 
##        30</code></pre>
<pre class="r"><code>t.test(C11T5[, &quot;Months30&quot;] * 1 + C11T5[, &quot;Months36&quot;] * -1 + C11T5[, &quot;Months42&quot;] * -1 + C11T5[, &quot;Months48&quot;] * 1)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  C11T5[, &quot;Months30&quot;] * 1 + C11T5[, &quot;Months36&quot;] * -1 + C11T5[,     &quot;Months42&quot;] * -1 + C11T5[, &quot;Months48&quot;] * 1
## t = -0.46749, df = 11, p-value = 0.6493
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -11.416264   7.416264
## sample estimates:
## mean of x 
##        -2</code></pre>
<pre class="r"><code>t.test(C11T5[, &quot;Months30&quot;] * -1 + C11T5[, &quot;Months36&quot;] * 3 + C11T5[, &quot;Months42&quot;] * -3 + C11T5[, &quot;Months48&quot;] * 1)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  C11T5[, &quot;Months30&quot;] * -1 + C11T5[, &quot;Months36&quot;] * 3 + C11T5[,     &quot;Months42&quot;] * -3 + C11T5[, &quot;Months48&quot;] * 1
## t = 0, df = 11, p-value = 1
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -12.69584  12.69584
## sample estimates:
## mean of x 
##         0</code></pre>
</div>
<div id="short-way" class="section level3">
<h3>Short way</h3>
<blockquote>
<p><code>%*%</code> conducts a dot product of the two matrices (i.e., a time point and a contrast). This means it sums up the products of the corresponding entries of the two sequences of numbers. This is equivalent to the method above: Apply contrast weights to each column and add up the results.</p>
</blockquote>
<pre class="r"><code>t.test(as.matrix(C11T5[, -5]) %*% linear)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  as.matrix(C11T5[, -5]) %*% linear
## t = 2.2414, df = 11, p-value = 0.04659
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   0.5403653 59.4596347
## sample estimates:
## mean of x 
##        30</code></pre>
<pre class="r"><code>t.test(as.matrix(C11T5[, -5]) %*% quadratic)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  as.matrix(C11T5[, -5]) %*% quadratic
## t = -0.46749, df = 11, p-value = 0.6493
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -11.416264   7.416264
## sample estimates:
## mean of x 
##        -2</code></pre>
<pre class="r"><code>t.test(as.matrix(C11T5[, -5]) %*% cubic)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  as.matrix(C11T5[, -5]) %*% cubic
## t = 0, df = 11, p-value = 1
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -12.69584  12.69584
## sample estimates:
## mean of x 
##         0</code></pre>
</div>
</div>
<div id="custom-function-for-testing-via-both-methods" class="section level2">
<h2>Custom function for testing via both methods</h2>
<blockquote>
<p>This function takes “wide-form” data and a contrast as input and it outputs a matrix with two columns: p-values and which testing approach they came from, pooled or separate error term.</p>
</blockquote>
<pre class="r"><code>wscont &lt;- function(wdata, cont) {
  # long data
  ldata &lt;- stack(wdata)
  colnames(ldata) &lt;- c(&quot;y&quot;, &quot;t&quot;)
  
  # add id factor; make time a factor
  ldata$id &lt;- factor(rep(1:nrow(wdata), times = ncol(wdata)))
  ldata$t &lt;- factor(ldata$t, ordered = TRUE)
  
  # separate variance contrast p-value
  svp &lt;- t.test(as.matrix(wdata) %*% cont)$p.value
  
  # fit model
  fit &lt;- lm(y ~ t + id, data = ldata)
  
  # extract p-value from linear contrast
  pvp &lt;- summary.aov(fit, split = list(t = list(lin = 1)))[[1]][, &quot;Pr(&gt;F)&quot;][&quot;lin&quot;]
  
  # output
  cbind(sepvar = c(0, 1), p = c(pvp, svp))
}</code></pre>
</div>
</div>
<div id="simulate-false-positive-error-rates-that-result-from-both-approaches" class="section level1">
<h1>Simulate false positive error rates that result from both approaches</h1>
<blockquote>
<p>Maxwell, Dalaney, and Kelly (2018) refer to these approaches as the univariate approach and the multivariate approach. To make the labels more descriptive, I refer to each approach by the error term it uses: pooled vs. seperate (MDK2018 do too)</p>
</blockquote>
<div id="true-covariance-matrix" class="section level2">
<h2>True Covariance Matrix</h2>
<blockquote>
<p>Here’s an example of a covariance matrix that satisifes the homogenous change score variance assumption. All timepoint variances are equal and so are the covariances between timepoints = 0. This is also called an identitiy matrix.</p>
</blockquote>
<pre class="r"><code>(truecov &lt;- diag(1, 4, 4))</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    1    0    0
## [3,]    0    0    1    0
## [4,]    0    0    0    1</code></pre>
</div>
<div id="sample-data" class="section level2">
<h2>Sample data</h2>
<blockquote>
<p>Below I generate data that represent 10 subjects’ scores across 4 times points. The mean score at each timepoint is 0, and the correlations among timepoints stem from the true matrix I saved above.</p>
</blockquote>
<pre class="r"><code># set random seed so results can be reproduced
set.seed(8888)

dat &lt;- data.frame(mvrnorm(n = 10, mu = c(0, 0, 0, 0), Sigma = truecov))</code></pre>
</div>
<div id="sample-test" class="section level2">
<h2>Sample test</h2>
<blockquote>
<p>Here I test out the function I made above.</p>
</blockquote>
<pre class="r"><code>wscont(wdata = dat, cont = linear)</code></pre>
<pre><code>##     sepvar          p
## lin      0 0.12806078
##          1 0.04189252</code></pre>
</div>
<div id="generate-data-that-satisfy-compound-symmetry" class="section level2">
<h2>Generate data that satisfy compound symmetry</h2>
<div id="compound-symmetric-covariance-matrix" class="section level3">
<h3>Compound Symmetric Covariance Matrix</h3>
<pre class="r"><code>(cscovmat &lt;- diag(0.5, nrow = 4, ncol = 4) + 0.5)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]  1.0  0.5  0.5  0.5
## [2,]  0.5  1.0  0.5  0.5
## [3,]  0.5  0.5  1.0  0.5
## [4,]  0.5  0.5  0.5  1.0</code></pre>
</div>
<div id="save-true-means-over-time" class="section level3">
<h3>Save true means over time</h3>
<pre class="r"><code>truemeans &lt;- c(100, 100, 100, 100)</code></pre>
</div>
<div id="data" class="section level3">
<h3>Data</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Create a function returns a data.frame with the requested means and variance-covariance structure (compound symmetry in this case)<br />
</li>
<li>Save a data.frame with 10,000 n = 50 replications</li>
</ol>
</blockquote>
<pre class="r"><code># function
gendatafun &lt;- function(truecov, ms, n) {
  data.frame(mvrnorm(n = n, mu = ms, Sigma = truecov))
}

# data generation
simdata1 &lt;- do.call(rbind, replicate(n = 10000, gendatafun(truecov = cscovmat, ms = truemeans, n = 50), simplify = FALSE))</code></pre>
</div>
</div>
<div id="generate-data-with-the-same-variance-covariance-structure-as-example-data" class="section level2">
<h2>Generate data with the same variance-covariance structure as example data</h2>
<div id="save-covariance-matrix-from-data-used-in-chapter-11-table-5-of-maxwell-dalaney-and-kelley-2018" class="section level3">
<h3>Save covariance matrix from data used in Chapter 11, Table 5 of Maxwell, Dalaney, and Kelley (2018)</h3>
<pre class="r"><code>(c11t5covmat &lt;- cov(C11T5[, -5]))</code></pre>
<pre><code>##          Months30  Months36 Months42  Months48
## Months30 188.0000 154.36364 127.3636 121.18182
## Months36 154.3636 200.54545 143.6364  97.45455
## Months42 127.3636 143.63636 178.0000 168.09091
## Months48 121.1818  97.45455 168.0909 218.00000</code></pre>
</div>
<div id="data-1" class="section level3">
<h3>Data</h3>
<blockquote>
<p>These data have a variance-covariance structure like the Chapter 11, Table 5 data.</p>
</blockquote>
<pre class="r"><code># data generation
simdata2 &lt;- do.call(rbind, replicate(n = 10000, gendatafun(truecov = c11t5covmat, ms = truemeans, n = 50), simplify = FALSE))</code></pre>
</div>
</div>
<div id="add-replicate-ids" class="section level2">
<h2>Add replicate IDs</h2>
<blockquote>
<p>I’ll use this column later to subset each replication.</p>
</blockquote>
<pre class="r"><code>simdata1$i &lt;- rep(1:10000, each = 50)
simdata2$i &lt;- rep(1:10000, each = 50)</code></pre>
</div>
<div id="get-p-values-via-custom-wscont-function" class="section level2">
<h2>Get p-values via custom <code>wscont()</code> function</h2>
<blockquote>
<p>In this last line, I add a column, compsymm, that flags which population variance-covariance matrix the data came from (0 = C11T5 matrix, 1 = compound symmetry).</p>
</blockquote>
<pre class="r"><code>simp1 &lt;- 1:10000 %&gt;% map_df(function(rep) {
  simdata1 %&gt;% 
    filter(i == rep) %&gt;% 
    select(-5) %&gt;% 
    wscont(cont = linear) %&gt;% 
    as_tibble() %&gt;% 
    mutate(compsymm = 1)
})

simp2 &lt;- 1:10000 %&gt;% map_df(function(rep) {
  simdata2 %&gt;% 
    filter(i == rep) %&gt;% 
    select(-5) %&gt;% 
    wscont(cont = linear) %&gt;% 
    as_tibble() %&gt;% 
    mutate(compsymm = 0)
})</code></pre>
</div>
<div id="plot-false-positive-errors" class="section level2">
<h2>Plot false positive errors</h2>
<blockquote>
<p><em>p</em> &lt; .05 should only occur 5% of the time</p>
</blockquote>
<pre class="r"><code>bind_rows(simp1, simp2) %&gt;% 
  mutate(sig = ifelse(p &lt; .05, 1, 0)) %&gt;% 
  group_by(compsymm, sepvar) %&gt;% 
  summarise(falsepos = mean(sig)) %&gt;% 
  ungroup() %&gt;% 
  mutate(sepvar = sepvar %&gt;% recode(`0` = &quot;Pooled Error Term&quot;, `1` = &quot;Separate Error Term&quot;),
         compsymm = compsymm %&gt;% recode(`0` = &quot;Unequal Variances &amp; Covariances&quot;, `1` = &quot;Compound Symmetry&quot;)) %&gt;% 
  ggplot(mapping = aes(x = compsymm, y = falsepos, fill = sepvar)) +
  geom_bar(stat = &quot;identity&quot;, position = position_dodge(0.9)) +
  geom_hline(yintercept = 0.05, color = &quot;red&quot;) +
  scale_y_continuous(breaks = seq(0, 0.5, 0.01)) +
  theme_bw() +
  theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/post/nrg07/nrg07_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
<div id="interpretation-1" class="section level2">
<h2>Interpretation</h2>
<blockquote>
<p>Above, I generated 20,000 datasets with known population variance-covariance matrices and columns means. For each dataset, I tested the linear contrast across time using both error term approaches and saved the <em>p</em>-values. When the true population variance-covariance matrice satisfied compound symmetry, both approaches’ contrast test statistics give practically the same results, on average, but when the true population variance-covariance matrice violated compound symmetry – a very common scenario in real datasets – the pooled error term test statistics resulted in false positives over 14% of the time; the separate error term test statistics resulted in false positives only 5% of the time.</p>
</blockquote>
</div>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<blockquote>
<ul>
<li>Boik, R. J. (1981). A priori tests in repeated measures designs: Effects of nonsphericity. <em>Psychometrika, 46</em>(3), 241-255.<br />
</li>
<li>Maxwell, S. E., Delaney, H. D., &amp; Kelley, K. (2018). <em>Designing experiments and analyzing data: A model comparison perspective</em> (3rd ed.). Routledge.<br />
</li>
<li><a href="https://designingexperiments.com/">DESIGNING EXPERIMENTS AND ANALYZING DATA</a> includes computer code for analyses from their book, Shiny apps, and more.</li>
</ul>
</blockquote>
</div>
<div id="general-word-of-caution" class="section level1">
<h1>General word of caution</h1>
<blockquote>
<p>Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don’t assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.</p>
</blockquote>
</div>
