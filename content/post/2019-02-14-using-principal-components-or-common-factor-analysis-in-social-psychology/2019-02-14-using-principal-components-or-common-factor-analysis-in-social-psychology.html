---
title: Using Principal Components or Common Factor Analysis in Social Psychology
author: Nick Michalak
date: '2019-02-14'
slug: using-principal-components-or-common-factor-analysis-in-social-psychology
categories:
  - common factor analysis
  - tutorial
  - principal components analysis
  - R
  - measurement
  - demonstration
  - reliability
tags:
  - common factor analysis
  - tutorial
  - principal component analysis
  - R
  - measurement
  - reliability
  - demonstration
image:
  caption: ''
  focal_point: ''
---



<p>Note. I’m not done with this post but it’s fun to build a new website and see your posts appear so pretty online … so here it is!</p>
<div id="multidimensional-scaling" class="section level1">
<h1>Multidimensional Scaling</h1>
<blockquote>
<p>Multidimensional scaling is an exploratory technique that uses distances or disimilarities between objects to create a multidimensional representation of those objects in metric space. In other words, multidimensional scaling uses data about the distance (e.g., miles between cities) or disimilarity (e.g., how (dis)similar are apples and tomatoes?) among a set of objects to “search” for some metric space that represents those objects and their relations to each other. Metric space is a fancy term for a set of objects and some metric that satisfy a list of axioms.</p>
</blockquote>
<blockquote>
<p>Here is one way to represent the distance (d) axioms:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><span class="math display">\[d_{ij} &gt; d_{ii} = 0 \text{,  for } i \neq j.\]</span></li>
<li><span class="math display">\[d_{ij} = d_{ji}.\]</span></li>
<li><span class="math display">\[d_{ij} \leq d_{ik} + d_{kj}.\]</span></li>
</ol>
<blockquote>
<p>Here is one way to represent the similarity (s) axioms:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><span class="math display">\[s_{ii} &gt; s_{ij}.\]</span></li>
<li><span class="math display">\[s_{ij} = s_ji.\]</span></li>
<li><span class="math display">\[\text{a large } s_{ij} \text{ implies } d_{ik} \approx d_{kj}.\]</span></li>
</ol>
<blockquote>
<p>The idea here is that the data and the axioms constrain the solution you get. For example, if you’re making a map of U.S. cities—and you want a accurate map of the cities—then including 100,000 cities in your analysis will place more constraints on your solution than including 10 cities. More contraints in this case will give you a better picture of 10 cities and their relation to each other (e.g., New York is north of Miami) than fewer constraints.<br />
This should be more clear in the example below.</p>
</blockquote>
<div id="install-packages-andor-load-libraries" class="section level2">
<h2>Install packages and/or load libraries</h2>
<blockquote>
<p>I’ll use these packages throughout this post.</p>
</blockquote>
<pre class="r"><code># install.packages(&quot;tidyverse&quot;)
# install.packages(&quot;knitr&quot;)
# install.packages(&quot;haven&quot;)
# install.packages(&quot;maps&quot;)
# install.packages(&quot;psych&quot;)

library(tidyverse)
library(knitr)
library(haven)
library(maps)
library(psych)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<blockquote>
<p><code>help(&quot;UScitiesD&quot;)</code><br />
“UScitiesD gives “straight line” distances between 10 cities in the US.”</p>
</blockquote>
<pre class="r"><code>UScitiesD %&gt;% 
  as.matrix() %&gt;% 
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Atlanta</th>
<th align="right">Chicago</th>
<th align="right">Denver</th>
<th align="right">Houston</th>
<th align="right">LosAngeles</th>
<th align="right">Miami</th>
<th align="right">NewYork</th>
<th align="right">SanFrancisco</th>
<th align="right">Seattle</th>
<th align="right">Washington.DC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atlanta</td>
<td align="right">0</td>
<td align="right">587</td>
<td align="right">1212</td>
<td align="right">701</td>
<td align="right">1936</td>
<td align="right">604</td>
<td align="right">748</td>
<td align="right">2139</td>
<td align="right">2182</td>
<td align="right">543</td>
</tr>
<tr class="even">
<td>Chicago</td>
<td align="right">587</td>
<td align="right">0</td>
<td align="right">920</td>
<td align="right">940</td>
<td align="right">1745</td>
<td align="right">1188</td>
<td align="right">713</td>
<td align="right">1858</td>
<td align="right">1737</td>
<td align="right">597</td>
</tr>
<tr class="odd">
<td>Denver</td>
<td align="right">1212</td>
<td align="right">920</td>
<td align="right">0</td>
<td align="right">879</td>
<td align="right">831</td>
<td align="right">1726</td>
<td align="right">1631</td>
<td align="right">949</td>
<td align="right">1021</td>
<td align="right">1494</td>
</tr>
<tr class="even">
<td>Houston</td>
<td align="right">701</td>
<td align="right">940</td>
<td align="right">879</td>
<td align="right">0</td>
<td align="right">1374</td>
<td align="right">968</td>
<td align="right">1420</td>
<td align="right">1645</td>
<td align="right">1891</td>
<td align="right">1220</td>
</tr>
<tr class="odd">
<td>LosAngeles</td>
<td align="right">1936</td>
<td align="right">1745</td>
<td align="right">831</td>
<td align="right">1374</td>
<td align="right">0</td>
<td align="right">2339</td>
<td align="right">2451</td>
<td align="right">347</td>
<td align="right">959</td>
<td align="right">2300</td>
</tr>
<tr class="even">
<td>Miami</td>
<td align="right">604</td>
<td align="right">1188</td>
<td align="right">1726</td>
<td align="right">968</td>
<td align="right">2339</td>
<td align="right">0</td>
<td align="right">1092</td>
<td align="right">2594</td>
<td align="right">2734</td>
<td align="right">923</td>
</tr>
<tr class="odd">
<td>NewYork</td>
<td align="right">748</td>
<td align="right">713</td>
<td align="right">1631</td>
<td align="right">1420</td>
<td align="right">2451</td>
<td align="right">1092</td>
<td align="right">0</td>
<td align="right">2571</td>
<td align="right">2408</td>
<td align="right">205</td>
</tr>
<tr class="even">
<td>SanFrancisco</td>
<td align="right">2139</td>
<td align="right">1858</td>
<td align="right">949</td>
<td align="right">1645</td>
<td align="right">347</td>
<td align="right">2594</td>
<td align="right">2571</td>
<td align="right">0</td>
<td align="right">678</td>
<td align="right">2442</td>
</tr>
<tr class="odd">
<td>Seattle</td>
<td align="right">2182</td>
<td align="right">1737</td>
<td align="right">1021</td>
<td align="right">1891</td>
<td align="right">959</td>
<td align="right">2734</td>
<td align="right">2408</td>
<td align="right">678</td>
<td align="right">0</td>
<td align="right">2329</td>
</tr>
<tr class="even">
<td>Washington.DC</td>
<td align="right">543</td>
<td align="right">597</td>
<td align="right">1494</td>
<td align="right">1220</td>
<td align="right">2300</td>
<td align="right">923</td>
<td align="right">205</td>
<td align="right">2442</td>
<td align="right">2329</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<div id="represent-the-distance-matrix-with-colors" class="section level3">
<h3>Represent the distance matrix with colors</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Convert the distance object into a <code>data.frame</code>.<br />
</li>
<li>Restructure the <code>data.frame</code> so each distance value gets its own row, and each distance value corresponds to two city names (even cities paired with themselves, so distance = 0).<br />
</li>
<li>Plot the distance values on tiles that are colored by the size of the distance.</li>
</ol>
</blockquote>
<pre class="r"><code>UScitiesD %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  mutate(city1 = rownames(.)) %&gt;% 
  gather(key = city2, value = distance, -city1) %&gt;% 
  ggplot(mapping = aes(x = city1, y = city2, fill = distance, label = distance)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-3-1.png" width="1008" /></p>
</div>
<div id="metric-multidimensional-scaling-2-dimensions-like-a-map" class="section level3">
<h3>Metric Multidimensional Scaling (2-dimensions, like a map)</h3>
<blockquote>
<p><code>help(&quot;cmdscale&quot;)</code><br />
“Classical multidimensional scaling (MDS) of a data matrix. Also known as principal coordinates analysis (Gower, 1966).”</p>
</blockquote>
<pre class="r"><code># give it distance values and number of dimensions
# also ask for the function to output eigenvalues
cmdscale.fit1 &lt;- cmdscale(UScitiesD, k = 2, eig = TRUE)</code></pre>
</div>
<div id="plot-eigenvalues-sorted-by-size-i.e.-screeplot" class="section level3">
<h3>Plot eigenvalues sorted by size (i.e., screeplot)</h3>
<blockquote>
<p>You can use eigenvalues to give you a sense of how many dimensions can capture most of the information from original data. A “big” eigenvalue means a lot of information is contained in a given dimension. You can tell how big the eigenvalues are by looking a a plot of the digenvalues sorted by size. Analysts typically determine the number of dimensions to use by counting eigenvalues from left to right until the pattern of points flattens out (i.e., there is little leftover information contain in the remaining dimensions).</p>
</blockquote>
<pre class="r"><code>tibble(index = 1:length(cmdscale.fit1$eig),
       eigenvalue = cmdscale.fit1$eig) %&gt;% 
  ggplot(mapping = aes(x = index, y = eigenvalue)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::comma)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="store-points-from-the-fitted-configurationsolution-in-a-tibble-data.frame" class="section level3">
<h3>Store points from the fitted configuration/solution in a tibble <code>data.frame</code></h3>
<pre class="r"><code>cmdscale.data &lt;- tibble(city = rownames(cmdscale.fit1$points),
                        dimension1 = cmdscale.fit1$points[, 1],
                        dimension2 = cmdscale.fit1$points[, 2])</code></pre>
<div id="plot-the-solution" class="section level4">
<h4>Plot the solution</h4>
<pre class="r"><code>cmdscale.data %&gt;% 
  ggplot(mapping = aes(x = dimension2, y = dimension1, label = city)) +
  geom_point() +
  geom_label(nudge_y = 100)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="compare-the-solution-above-to-a-map-of-the-u.s.-below" class="section level4">
<h4>Compare the solution above to a map of the U.S. below</h4>
<blockquote>
<p>Because we only used pairwise distances among ten U.S. cities, the solution imperfectly represented real relations among these cities. For example, Miami seems to be in the right spot on the “Southeastern quadrant” of the plot, but New York and Washington D.C. are placed in the “Southwestern quadrant” of the plot.</p>
</blockquote>
<pre class="r"><code>map_data(&quot;state&quot;) %&gt;% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) +
  coord_quickmap()</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
</div>
</div>
<div id="measuring-variables-we-cant-observe" class="section level1">
<h1>Measuring variables we can’t observe</h1>
<blockquote>
<p>If we think psychological constructs like extraversion, social dominance orientation, or depression exist, then we should be able to measure them. We can’t measure these constructs directly like we can measure blood pressure or light absorption. But we can ask people questions that we believe index parts of what we mean by, say, extroversion (e.g., outgoing, sociable, acts like a leader). The questions we ask make up our measurement “test” (e.g., like a test of extroversion), and the scores on our test comprise two parts: truth and error (random or systematic). In other words, unobservable psychological constructs and error can explain the scores we can observe from the tests we administer. This is classical test theory.</p>
</blockquote>
</div>
<div id="partitioning-variance" class="section level1">
<h1>Partitioning variance</h1>
<blockquote>
<p>A factor analysis models the variability in item responses (e.g., responses to questions on a test). Some of that variability can be explained by the relationship between items; some can be explained by what is “special” about the items; and some can be explained by error, random or systematic.</p>
</blockquote>
<div id="common-variance" class="section level2">
<h2>Common Variance</h2>
<blockquote>
<p>Common variance refers to variability shared among the items (i.e., what can be explained by inter-item correlations)</p>
</blockquote>
<div id="communality" class="section level3">
<h3>Communality</h3>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &gt; Communality (* h^2 <em>) is one definition of common variance and can take values between 0 and 1: </em> h^2 * = means no observed variability can be explained, and 1 means all observed variability can be explained ======= &gt; Communality (* h^2 <em>) is one definition of common variance and can take values between 0 and 1: </em> h^2 * = means no observed variability can be explained, and 1 means all observed variability can be explained.</p>
</div>
</div>
<div id="unique-variance" class="section level2">
<h2>Unique Variance</h2>
<blockquote>
<p>Unique variance refers to not-common variance. It has two parts: specific and error.</p>
</blockquote>
<div id="specific-variance" class="section level3">
<h3>Specific Variance</h3>
<blockquote>

</blockquote>
</div>
<div id="error-variance" class="section level3">
<h3>Error Variance</h3>
</div>
</div>
</div>
<div id="spss-anxiety-questionnaire-saq" class="section level1">
<h1>SPSS Anxiety Questionnaire (SAQ)</h1>
<div id="description" class="section level2">
<h2>Description</h2>
<div id="read-spss-anxiety-questionnaire-data" class="section level3">
<h3>Read SPSS Anxiety Questionnaire data</h3>
<blockquote>
<p>You can download these data [<a href="https://github.com/nmmichalak/academic-kickstart/raw/master/static/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/data/SAQ.sav"><strong>SAQ.sav</strong></a>]</p>
</blockquote>
<pre class="r"><code>saq &lt;- read_spss(file = &quot;data/SAQ.sav&quot;)</code></pre>
</div>
<div id="correlation-matrix" class="section level3">
<h3>Correlation matrix</h3>
<pre class="r"><code>saq %&gt;% 
  cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% 
  round(2) %&gt;% 
  as.data.frame() %&gt;% 
  rownames_to_column() %&gt;%
  rename(item1 = rowname) %&gt;% 
  gather(key = item2, value = r, -item1) %&gt;% 
  ggplot(mapping = aes(x = item1, y = item2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-10-1.png" width="1512" /></p>
</div>
<div id="histograms" class="section level3">
<h3>Histograms</h3>
<pre class="r"><code>saq %&gt;% 
  gather(key = variable, value = response) %&gt;% 
  ggplot(mapping = aes(x = response)) +
  geom_histogram(binwidth = 1, color = &quot;white&quot;) +
  facet_wrap(facets = ~ variable)</code></pre>
<pre><code>## Warning: attributes are not identical across measure variables;
## they will be dropped</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="parallel-analysis" class="section level3">
<h3>Parallel Analysis</h3>
<pre class="r"><code>fa.parallel(saq, fm = &quot;minres&quot;, fa = &quot;both&quot;)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre><code>## Parallel analysis suggests that the number of factors =  6  and the number of components =  4</code></pre>
</div>
</div>
</div>
<div id="principal-components-analysis" class="section level1">
<h1>Principal Components Analysis</h1>
<div id="all-items" class="section level2">
<h2>all items</h2>
<pre class="r"><code>principal(saq, rotate = &quot;none&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Principal Components Analysis
## Call: principal(r = saq, rotate = &quot;none&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##      V   PC1    h2   u2 com
## q18 18  0.70 0.492 0.51   1
## q07  7  0.69 0.469 0.53   1
## q16 16  0.68 0.461 0.54   1
## q13 13  0.67 0.453 0.55   1
## q12 12  0.67 0.447 0.55   1
## q21 21  0.66 0.432 0.57   1
## q14 14  0.66 0.430 0.57   1
## q11 11  0.65 0.426 0.57   1
## q17 17  0.64 0.414 0.59   1
## q04  4  0.63 0.403 0.60   1
## q03  3 -0.63 0.396 0.60   1
## q15 15  0.59 0.351 0.65   1
## q01  1  0.59 0.343 0.66   1
## q06  6  0.56 0.316 0.68   1
## q05  5  0.56 0.309 0.69   1
## q08  8  0.55 0.301 0.70   1
## q10 10  0.44 0.191 0.81   1
## q20 20  0.44 0.190 0.81   1
## q19 19 -0.43 0.182 0.82   1
## q02  2 -0.30 0.092 0.91   1
## q22 22 -0.30 0.091 0.91   1
## q09  9 -0.28 0.081 0.92   1
## q23 23 -0.14 0.021 0.98   1
## 
##                 PC1
## SS loadings    7.29
## Proportion Var 0.32
## 
## Mean item complexity =  1
## Test of the hypothesis that 1 component is sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.07 
##  with the empirical chi square  6633.48  with prob &lt;  0 
## 
## Fit based upon off diagonal values = 0.94</code></pre>
<pre class="r"><code>fa(saq, rotate = &quot;none&quot;, fm = &quot;pa&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Factor Analysis using method =  pa
## Call: fa(r = saq, rotate = &quot;none&quot;, fm = &quot;pa&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##      V   PA1    h2   u2 com
## q18 18  0.68 0.464 0.54   1
## q07  7  0.66 0.440 0.56   1
## q16 16  0.66 0.430 0.57   1
## q13 13  0.65 0.422 0.58   1
## q12 12  0.64 0.416 0.58   1
## q21 21  0.63 0.400 0.60   1
## q14 14  0.63 0.398 0.60   1
## q11 11  0.63 0.392 0.61   1
## q17 17  0.62 0.381 0.62   1
## q04  4  0.61 0.369 0.63   1
## q03  3 -0.60 0.360 0.64   1
## q15 15  0.56 0.317 0.68   1
## q01  1  0.56 0.310 0.69   1
## q06  6  0.53 0.283 0.72   1
## q05  5  0.52 0.275 0.73   1
## q08  8  0.52 0.269 0.73   1
## q10 10  0.41 0.164 0.84   1
## q20 20  0.40 0.163 0.84   1
## q19 19 -0.39 0.156 0.84   1
## q02  2 -0.28 0.076 0.92   1
## q22 22 -0.27 0.075 0.92   1
## q09  9 -0.26 0.066 0.93   1
## q23 23 -0.13 0.017 0.98   1
## 
##                 PA1
## SS loadings    6.64
## Proportion Var 0.29
## 
## Mean item complexity =  1
## Test of the hypothesis that 1 factor is sufficient.
## 
## The degrees of freedom for the null model are  253  and the objective function was  7.55 with Chi Square of  19334.49
## The degrees of freedom for the model are 230  and the objective function was  1.74 
## 
## The root mean square of the residuals (RMSR) is  0.07 
## The df corrected root mean square of the residuals is  0.07 
## 
## The harmonic number of observations is  2571 with the empirical chi square  5547.21  with prob &lt;  0 
## The total number of observations was  2571  with Likelihood Chi Square =  4463.05  with prob &lt;  0 
## 
## Tucker Lewis Index of factoring reliability =  0.756
## RMSEA index =  0.085  and the 90 % confidence intervals are  0.082 0.087
## BIC =  2657.08
## Fit based upon off diagonal values = 0.95
## Measures of factor score adequacy             
##                                                    PA1
## Correlation of (regression) scores with factors   0.96
## Multiple R square of scores with factors          0.91
## Minimum correlation of possible factor scores     0.83</code></pre>
<pre class="r"><code># plot
principal(saq, rotate = &quot;none&quot;) %&gt;% 
  fa.diagram(sort = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="components" class="section level2">
<h2>4 components</h2>
<pre class="r"><code>principal(saq, nfactors = 4, rotate = &quot;varimax&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Principal Components Analysis
## Call: principal(r = saq, nfactors = 4, rotate = &quot;varimax&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##     item   RC3   RC1   RC4   RC2   h2   u2 com
## q06    6  0.80 -0.01  0.10 -0.07 0.65 0.35 1.0
## q18   18  0.68  0.33  0.13 -0.08 0.60 0.40 1.5
## q13   13  0.65  0.23  0.23 -0.10 0.54 0.46 1.6
## q07    7  0.64  0.33  0.16 -0.08 0.55 0.45 1.7
## q14   14  0.58  0.36  0.14 -0.07 0.49 0.51 1.8
## q10   10  0.55  0.00  0.13 -0.12 0.33 0.67 1.2
## q15   15  0.46  0.22  0.29 -0.19 0.38 0.62 2.6
## q20   20 -0.04  0.68  0.07 -0.14 0.48 0.52 1.1
## q21   21  0.29  0.66  0.16 -0.07 0.55 0.45 1.5
## q03    3 -0.20 -0.57 -0.18  0.37 0.53 0.47 2.3
## q12   12  0.47  0.52  0.09 -0.08 0.51 0.49 2.1
## q04    4  0.32  0.52  0.31  0.04 0.47 0.53 2.4
## q16   16  0.33  0.51  0.31 -0.12 0.49 0.51 2.6
## q01    1  0.24  0.50  0.36  0.06 0.43 0.57 2.4
## q05    5  0.32  0.43  0.24  0.01 0.34 0.66 2.5
## q08    8  0.13  0.17  0.83  0.01 0.74 0.26 1.1
## q17   17  0.27  0.22  0.75 -0.04 0.68 0.32 1.5
## q11   11  0.26  0.21  0.75 -0.14 0.69 0.31 1.5
## q09    9 -0.09 -0.20  0.12  0.65 0.48 0.52 1.3
## q22   22 -0.19  0.03 -0.10  0.65 0.46 0.54 1.2
## q23   23 -0.02  0.17 -0.20  0.59 0.41 0.59 1.4
## q02    2 -0.01 -0.34  0.07  0.54 0.41 0.59 1.7
## q19   19 -0.15 -0.37 -0.03  0.43 0.34 0.66 2.2
## 
##                        RC3  RC1  RC4  RC2
## SS loadings           3.73 3.34 2.55 1.95
## Proportion Var        0.16 0.15 0.11 0.08
## Cumulative Var        0.16 0.31 0.42 0.50
## Proportion Explained  0.32 0.29 0.22 0.17
## Cumulative Proportion 0.32 0.61 0.83 1.00
## 
## Mean item complexity =  1.8
## Test of the hypothesis that 4 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.06 
##  with the empirical chi square  4006.15  with prob &lt;  0 
## 
## Fit based upon off diagonal values = 0.96</code></pre>
<pre class="r"><code># plot
principal(saq, nfactors = 4, rotate = &quot;varimax&quot;) %&gt;% 
  fa.diagram(sort = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="common-factor-analysis" class="section level1">
<h1>Common Factor Analysis</h1>
<div id="all-items-1" class="section level2">
<h2>all items</h2>
<pre class="r"><code>fa(saq, fm = &quot;minres&quot;, rotate = &quot;none&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Factor Analysis using method =  minres
## Call: fa(r = saq, rotate = &quot;none&quot;, fm = &quot;minres&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##      V   MR1    h2   u2 com
## q18 18  0.68 0.464 0.54   1
## q07  7  0.66 0.440 0.56   1
## q16 16  0.66 0.430 0.57   1
## q13 13  0.65 0.422 0.58   1
## q12 12  0.64 0.416 0.58   1
## q21 21  0.63 0.400 0.60   1
## q14 14  0.63 0.398 0.60   1
## q11 11  0.63 0.392 0.61   1
## q17 17  0.62 0.381 0.62   1
## q04  4  0.61 0.369 0.63   1
## q03  3 -0.60 0.360 0.64   1
## q15 15  0.56 0.317 0.68   1
## q01  1  0.56 0.310 0.69   1
## q06  6  0.53 0.283 0.72   1
## q05  5  0.52 0.275 0.73   1
## q08  8  0.52 0.269 0.73   1
## q10 10  0.41 0.164 0.84   1
## q20 20  0.40 0.163 0.84   1
## q19 19 -0.39 0.156 0.84   1
## q02  2 -0.28 0.076 0.92   1
## q22 22 -0.27 0.075 0.92   1
## q09  9 -0.26 0.066 0.93   1
## q23 23 -0.13 0.017 0.98   1
## 
##                 MR1
## SS loadings    6.64
## Proportion Var 0.29
## 
## Mean item complexity =  1
## Test of the hypothesis that 1 factor is sufficient.
## 
## The degrees of freedom for the null model are  253  and the objective function was  7.55 with Chi Square of  19334.49
## The degrees of freedom for the model are 230  and the objective function was  1.74 
## 
## The root mean square of the residuals (RMSR) is  0.07 
## The df corrected root mean square of the residuals is  0.07 
## 
## The harmonic number of observations is  2571 with the empirical chi square  5547.21  with prob &lt;  0 
## The total number of observations was  2571  with Likelihood Chi Square =  4463.05  with prob &lt;  0 
## 
## Tucker Lewis Index of factoring reliability =  0.756
## RMSEA index =  0.085  and the 90 % confidence intervals are  0.082 0.087
## BIC =  2657.08
## Fit based upon off diagonal values = 0.95
## Measures of factor score adequacy             
##                                                    MR1
## Correlation of (regression) scores with factors   0.96
## Multiple R square of scores with factors          0.91
## Minimum correlation of possible factor scores     0.83</code></pre>
<pre class="r"><code># plot
fa(saq, fm = &quot;minres&quot;, rotate = &quot;none&quot;) %&gt;% 
  fa.diagram(sort = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="compare-loadings-to-those-from-pca" class="section level2">
<h2>compare loadings to those from PCA</h2>
<pre class="r"><code>tibble(item = names(principal(saq, rotate = &quot;none&quot;)$loadings[, 1]),
       pca = principal(saq, rotate = &quot;none&quot;)$loadings[, 1],
       fa = fa(saq, rotate = &quot;none&quot;, fm = &quot;minres&quot;)$loadings[, 1]) %&gt;% 
  gather(key = method, value = loading, pca, fa) %&gt;% 
  ggplot(mapping = aes(x = item, y = loading, fill = method)) +
  geom_bar(stat = &quot;identity&quot;, position = position_dodge(width = 0.90)) +
  theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="factors" class="section level2">
<h2>6 factors</h2>
<pre class="r"><code>fa(saq, nfactors = 6, fm = &quot;minres&quot;, rotate = &quot;oblimin&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Loading required namespace: GPArotation</code></pre>
<pre><code>## Factor Analysis using method =  minres
## Call: fa(r = saq, nfactors = 6, rotate = &quot;oblimin&quot;, fm = &quot;minres&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##     item   MR1   MR4   MR5   MR2   MR3   MR6   h2   u2 com
## q06    6  0.77  0.02 -0.09  0.00 -0.08  0.09 0.57 0.43 1.1
## q18   18  0.60  0.01  0.14 -0.04  0.14 -0.03 0.56 0.44 1.2
## q13   13  0.53  0.14  0.12 -0.08  0.03 -0.04 0.49 0.51 1.3
## q07    7  0.49  0.03  0.05  0.00  0.21  0.11 0.50 0.50 1.5
## q14   14  0.38  0.01  0.17 -0.03  0.14  0.10 0.42 0.58 1.9
## q12   12  0.31 -0.02  0.25 -0.07  0.26  0.00 0.45 0.55 3.1
## q10   10  0.28  0.01  0.17 -0.08 -0.14  0.16 0.22 0.78 3.1
## q08    8 -0.06  0.85  0.00  0.06  0.01  0.00 0.67 0.33 1.0
## q11   11  0.07  0.74 -0.02 -0.10  0.00  0.01 0.63 0.37 1.1
## q17   17  0.06  0.63  0.09  0.03  0.02  0.06 0.57 0.43 1.1
## q01    1  0.00  0.08  0.71  0.01 -0.04 -0.05 0.51 0.49 1.0
## q16   16 -0.03  0.01  0.53 -0.08  0.05  0.27 0.54 0.46 1.6
## q05    5  0.11  0.02  0.48 -0.01  0.02  0.02 0.34 0.66 1.1
## q04    4  0.10  0.09  0.42  0.04  0.15  0.06 0.42 0.58 1.5
## q09    9  0.02  0.09 -0.02  0.59  0.01 -0.07 0.35 0.65 1.1
## q22   22 -0.11 -0.08  0.03  0.50  0.11  0.03 0.26 0.74 1.3
## q02    2  0.05  0.01  0.03  0.45 -0.16 -0.03 0.26 0.74 1.3
## q23   23 -0.02 -0.12  0.08  0.36  0.09  0.04 0.12 0.88 1.5
## q03    3 -0.01 -0.08 -0.20  0.35 -0.26 -0.03 0.46 0.54 2.7
## q19   19 -0.06 -0.03 -0.02  0.34 -0.19 -0.01 0.25 0.75 1.7
## q21   21  0.14  0.06  0.04  0.02  0.65  0.03 0.60 0.40 1.1
## q20   20 -0.11  0.06 -0.04 -0.09  0.59  0.04 0.37 0.63 1.2
## q15   15  0.03  0.04  0.00 -0.01  0.00  0.80 0.70 0.30 1.0
## 
##                        MR1  MR4  MR5  MR2  MR3  MR6
## SS loadings           2.37 2.01 1.94 1.40 1.50 1.06
## Proportion Var        0.10 0.09 0.08 0.06 0.07 0.05
## Cumulative Var        0.10 0.19 0.27 0.34 0.40 0.45
## Proportion Explained  0.23 0.20 0.19 0.14 0.15 0.10
## Cumulative Proportion 0.23 0.43 0.62 0.75 0.90 1.00
## 
##  With factor correlations of 
##       MR1   MR4   MR5   MR2   MR3   MR6
## MR1  1.00  0.45  0.48 -0.29  0.39  0.49
## MR4  0.45  1.00  0.54 -0.17  0.38  0.44
## MR5  0.48  0.54  1.00 -0.29  0.53  0.43
## MR2 -0.29 -0.17 -0.29  1.00 -0.35 -0.34
## MR3  0.39  0.38  0.53 -0.35  1.00  0.33
## MR6  0.49  0.44  0.43 -0.34  0.33  1.00
## 
## Mean item complexity =  1.5
## Test of the hypothesis that 6 factors are sufficient.
## 
## The degrees of freedom for the null model are  253  and the objective function was  7.55 with Chi Square of  19334.49
## The degrees of freedom for the model are 130  and the objective function was  0.23 
## 
## The root mean square of the residuals (RMSR) is  0.02 
## The df corrected root mean square of the residuals is  0.02 
## 
## The harmonic number of observations is  2571 with the empirical chi square  364.32  with prob &lt;  4.4e-24 
## The total number of observations was  2571  with Likelihood Chi Square =  577.8  with prob &lt;  1.1e-57 
## 
## Tucker Lewis Index of factoring reliability =  0.954
## RMSEA index =  0.037  and the 90 % confidence intervals are  0.034 0.04
## BIC =  -442.96
## Fit based upon off diagonal values = 1
## Measures of factor score adequacy             
##                                                    MR1  MR4  MR5  MR2  MR3
## Correlation of (regression) scores with factors   0.90 0.92 0.88 0.82 0.85
## Multiple R square of scores with factors          0.81 0.84 0.78 0.67 0.73
## Minimum correlation of possible factor scores     0.63 0.68 0.56 0.33 0.46
##                                                    MR6
## Correlation of (regression) scores with factors   0.86
## Multiple R square of scores with factors          0.75
## Minimum correlation of possible factor scores     0.49</code></pre>
<pre class="r"><code># plot
fa(saq, nfactors = 6, fm = &quot;minres&quot;, rotate = &quot;oblimin&quot;) %&gt;% 
  fa.diagram(sort = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<blockquote>
<ul>
<li>Gonzalez, R. (February, 2016). <em>Lecture Notes #10: MDS &amp; Tree Structures.</em> Retrieved from <a href="http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf" class="uri">http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf</a> on February 17, 2019.<br />
</li>
<li>Revelle, W. (n.d.). <em>An introduction to psychometric theory with applications in R.</em> Retreived from <a href="http://www.personality-project.org/r/book/" class="uri">http://www.personality-project.org/r/book/</a> on February 17, 2019.</li>
<li>A practical introduction to factor analysis: exploratory factor analysis. <em>UCLA: Statistical Consulting Group.</em> Retreived from <a href="https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/" class="uri">https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/</a> on February 17, 2019.</li>
</ul>
</blockquote>
</div>
<div id="general-word-of-caution" class="section level1">
<h1>General word of caution</h1>
<blockquote>
<p>Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don’t assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.</p>
</blockquote>
</div>
