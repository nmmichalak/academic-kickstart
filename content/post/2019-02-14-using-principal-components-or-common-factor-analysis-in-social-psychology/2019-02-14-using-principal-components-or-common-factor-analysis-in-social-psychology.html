---
title: Using Principal Components or Common Factor Analysis in Social Psychology
author: Nick Michalak
date: '2019-02-14'
slug: using-principal-components-or-common-factor-analysis-in-social-psychology
categories:
  - common factor analysis
  - tutorial
  - principal components analysis
  - multidimensional scaling
  - R
  - measurement
  - demonstration
  - reliability
tags:
  - common factor analysis
  - tutorial
  - principal component analysis
  - multidimensional scaling
  - R
  - measurement
  - reliability
  - demonstration
image:
  caption: ''
  focal_point: ''
---



<div id="multidimensional-scaling-the-precursor-to-principal-components-analysis-common-factor-analysis-and-related-techniques" class="section level1">
<h1>Multidimensional Scaling, the precursor to Principal Components Analysis, Common Factor Analysis, and related techniques</h1>
<blockquote>
<p>Multidimensional scaling is an exploratory technique that uses distances or disimilarities between objects to create a multidimensional representation of those objects in metric space. In other words, multidimensional scaling uses data about the distance (e.g., miles between cities) or disimilarity (e.g., how (dis)similar are apples and tomatoes?) among a set of objects to “search” for some metric space that represents those objects and their relations to each other. Metric space is a fancy term for a set of objects and some metric that satisfy a list of axioms.</p>
</blockquote>
<blockquote>
<p>Here is one way to represent the distance (d) axioms:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><span class="math display">\[d_{ij} &gt; d_{ii} = 0 \text{,  for } i \neq j.\]</span></li>
<li><span class="math display">\[d_{ij} = d_{ji}.\]</span></li>
<li><span class="math display">\[d_{ij} \leq d_{ik} + d_{kj}.\]</span></li>
</ol>
<blockquote>
<p>Here is one way to represent the similarity (s) axioms:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><span class="math display">\[s_{ii} &gt; s_{ij}.\]</span></li>
<li><span class="math display">\[s_{ij} = s_ji.\]</span></li>
<li><span class="math display">\[\text{a large } s_{ij} \text{ implies } d_{ik} \approx d_{kj}.\]</span></li>
</ol>
<blockquote>
<p>If you’re familiar with the Pythagorean Theorem <span class="math display">\[c = \sqrt{a^2 + b^2}\]</span>, and you have a sense of what “similar” means, then these axioms should be intutuive to you. With regard to Multidimensional Scaling, the big idea here is that the data and the axioms constrain the solution you get. For example, if you’re making a map of U.S. cities—and you want an accurate map of the cities—then including 100,000 cities in your analysis will place more constraints on your solution than including 10 cities. More contraints in this case will give you a better picture of 10 cities and their relation to each other (e.g., New York is north of Miami) than fewer constraints.</p>
</blockquote>
<blockquote>
<p>This should be more clear in the example below.</p>
</blockquote>
<div id="install-packages-andor-load-libraries" class="section level2">
<h2>Install packages and/or load libraries</h2>
<blockquote>
<p>I’ll use these packages throughout this post.</p>
</blockquote>
<pre class="r"><code># install.packages(&quot;tidyverse&quot;)
# install.packages(&quot;knitr&quot;)
# install.packages(&quot;haven&quot;)
# install.packages(&quot;maps&quot;)
# install.packages(&quot;psych&quot;)

library(tidyverse)
library(knitr)
library(haven)
library(maps)
library(psych)

# use select from dplyr
select &lt;- dplyr::select</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<blockquote>
<p><code>help(&quot;UScitiesD&quot;)</code><br />
“UScitiesD gives “straight line” distances between 10 cities in the US.”</p>
</blockquote>
<pre class="r"><code>UScitiesD %&gt;% 
  as.matrix() %&gt;% 
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Atlanta</th>
<th align="right">Chicago</th>
<th align="right">Denver</th>
<th align="right">Houston</th>
<th align="right">LosAngeles</th>
<th align="right">Miami</th>
<th align="right">NewYork</th>
<th align="right">SanFrancisco</th>
<th align="right">Seattle</th>
<th align="right">Washington.DC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atlanta</td>
<td align="right">0</td>
<td align="right">587</td>
<td align="right">1212</td>
<td align="right">701</td>
<td align="right">1936</td>
<td align="right">604</td>
<td align="right">748</td>
<td align="right">2139</td>
<td align="right">2182</td>
<td align="right">543</td>
</tr>
<tr class="even">
<td>Chicago</td>
<td align="right">587</td>
<td align="right">0</td>
<td align="right">920</td>
<td align="right">940</td>
<td align="right">1745</td>
<td align="right">1188</td>
<td align="right">713</td>
<td align="right">1858</td>
<td align="right">1737</td>
<td align="right">597</td>
</tr>
<tr class="odd">
<td>Denver</td>
<td align="right">1212</td>
<td align="right">920</td>
<td align="right">0</td>
<td align="right">879</td>
<td align="right">831</td>
<td align="right">1726</td>
<td align="right">1631</td>
<td align="right">949</td>
<td align="right">1021</td>
<td align="right">1494</td>
</tr>
<tr class="even">
<td>Houston</td>
<td align="right">701</td>
<td align="right">940</td>
<td align="right">879</td>
<td align="right">0</td>
<td align="right">1374</td>
<td align="right">968</td>
<td align="right">1420</td>
<td align="right">1645</td>
<td align="right">1891</td>
<td align="right">1220</td>
</tr>
<tr class="odd">
<td>LosAngeles</td>
<td align="right">1936</td>
<td align="right">1745</td>
<td align="right">831</td>
<td align="right">1374</td>
<td align="right">0</td>
<td align="right">2339</td>
<td align="right">2451</td>
<td align="right">347</td>
<td align="right">959</td>
<td align="right">2300</td>
</tr>
<tr class="even">
<td>Miami</td>
<td align="right">604</td>
<td align="right">1188</td>
<td align="right">1726</td>
<td align="right">968</td>
<td align="right">2339</td>
<td align="right">0</td>
<td align="right">1092</td>
<td align="right">2594</td>
<td align="right">2734</td>
<td align="right">923</td>
</tr>
<tr class="odd">
<td>NewYork</td>
<td align="right">748</td>
<td align="right">713</td>
<td align="right">1631</td>
<td align="right">1420</td>
<td align="right">2451</td>
<td align="right">1092</td>
<td align="right">0</td>
<td align="right">2571</td>
<td align="right">2408</td>
<td align="right">205</td>
</tr>
<tr class="even">
<td>SanFrancisco</td>
<td align="right">2139</td>
<td align="right">1858</td>
<td align="right">949</td>
<td align="right">1645</td>
<td align="right">347</td>
<td align="right">2594</td>
<td align="right">2571</td>
<td align="right">0</td>
<td align="right">678</td>
<td align="right">2442</td>
</tr>
<tr class="odd">
<td>Seattle</td>
<td align="right">2182</td>
<td align="right">1737</td>
<td align="right">1021</td>
<td align="right">1891</td>
<td align="right">959</td>
<td align="right">2734</td>
<td align="right">2408</td>
<td align="right">678</td>
<td align="right">0</td>
<td align="right">2329</td>
</tr>
<tr class="even">
<td>Washington.DC</td>
<td align="right">543</td>
<td align="right">597</td>
<td align="right">1494</td>
<td align="right">1220</td>
<td align="right">2300</td>
<td align="right">923</td>
<td align="right">205</td>
<td align="right">2442</td>
<td align="right">2329</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<div id="represent-the-distance-matrix-with-colors" class="section level3">
<h3>Represent the distance matrix with colors</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Convert the distance object into a <code>data.frame</code>.<br />
</li>
<li>Restructure the <code>data.frame</code> so each distance value gets its own row, and each distance value corresponds to two city names (even cities paired with themselves, so distance = 0).<br />
</li>
<li>Plot the distance values on tiles that are colored by the size of the distance.</li>
</ol>
</blockquote>
<pre class="r"><code>UScitiesD %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  mutate(city1 = rownames(.)) %&gt;% 
  gather(key = city2, value = distance, -city1) %&gt;% 
  ggplot(mapping = aes(x = city1, y = city2, fill = distance, label = distance)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-3-1.png" width="1008" /></p>
<div id="interpretation" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>Darker tiles index that two cities are relatively far apart, and lighter tiles index that two cities are relatively close together (e.g., white tiles = a city is right next to itself)</p>
</blockquote>
</div>
</div>
<div id="metric-multidimensional-scaling-2-dimensions-like-a-map" class="section level3">
<h3>Metric Multidimensional Scaling (2-dimensions, like a map)</h3>
<blockquote>
<p><code>help(&quot;cmdscale&quot;)</code><br />
“Classical multidimensional scaling (MDS) of a data matrix. Also known as principal coordinates analysis (Gower, 1966).”</p>
</blockquote>
<pre class="r"><code># give it distance values and number of dimensions
# also ask for the function to output eigenvalues
cmdscale.fit1 &lt;- cmdscale(UScitiesD, k = 2, eig = TRUE)</code></pre>
</div>
<div id="plot-eigenvalues-sorted-by-size-i.e.-screeplot" class="section level3">
<h3>Plot eigenvalues sorted by size (i.e., screeplot)</h3>
<blockquote>
<p>You can use eigenvalues to give you a sense of how many dimensions can capture most of the information from original data. A “big” eigenvalue means a lot of information is contained in a given dimension. You can tell how big the eigenvalues are by looking a a plot of the digenvalues sorted by size. Analysts typically determine the number of dimensions to use by counting eigenvalues from left to right until the pattern of points flattens out (i.e., there is little leftover information contained in the remaining dimensions).</p>
</blockquote>
<pre class="r"><code>tibble(index = 1:length(cmdscale.fit1$eig),
       eigenvalue = cmdscale.fit1$eig) %&gt;% 
  ggplot(mapping = aes(x = index, y = eigenvalue)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::comma)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div id="interpretation-1" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>Because the lines stop dropping after 2-3 points, this suggest most of the information can be “captured” by 2-3 dimensions. To make a plot easier to interpret, I already asked for 2 dimensions.</p>
</blockquote>
</div>
</div>
<div id="store-points-from-the-fitted-configurationsolution-in-a-tibble-data.frame" class="section level3">
<h3>Store points from the fitted configuration/solution in a tibble <code>data.frame</code></h3>
<blockquote>
<p>The first and second dimension are stored as columns in the <code>points</code> object from the <code>cmdscale</code> function</p>
</blockquote>
<pre class="r"><code>cmdscale.data &lt;- tibble(city = rownames(cmdscale.fit1$points),
                        dimension1 = cmdscale.fit1$points[, 1],
                        dimension2 = cmdscale.fit1$points[, 2])</code></pre>
<div id="plot-the-solution" class="section level4">
<h4>Plot the solution</h4>
<blockquote>
<p>Note that I asked ggplot2 to plot the negative dimension scores to make the plot more interpretable compared to a map of the U.S. (i.e., real life)</p>
</blockquote>
<pre class="r"><code>cmdscale.data %&gt;% 
  ggplot(mapping = aes(x = -dimension1, y = -dimension2, label = city)) +
  geom_point() +
  geom_label(nudge_y = 100)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="compare-the-solution-above-to-a-map-of-the-u.s.-below" class="section level4">
<h4>Compare the solution above to a map of the U.S. below</h4>
<blockquote>
<p>Because we only used pairwise distances among ten U.S. cities, the solution imperfectly represented real relations among these cities. For example, Miami seems to be in the right spot on the “Southeastern quadrant” of the plot, it seems to sit on almost a horizontal line with Houston; in real life, Houston is northwest of Miami, not directly west like the solution implies.</p>
</blockquote>
<pre class="r"><code>map_data(&quot;state&quot;) %&gt;% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) +
  coord_quickmap()</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
</div>
</div>
<div id="principal-components-and-common-factor-anlayses-are-special-cases-of-multidimensional-scaling" class="section level1">
<h1>Principal Components and Common Factor Anlayses are special cases of Multidimensional Scaling</h1>
<blockquote>
<p>The major difference between Multidimensional Scaling and Principal Components and Common Factor Anlayses is that Multidimensional Scaling uses distances and (dis)similarity matrices and Principal Components and Common Factor Anlayses use covariance and correlation matrices. But all these techniques share common issues:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>How do you collect data (e.g., (dis)simlarities between items, likert responses to a question)<br />
</li>
<li>How do you rotate the solution?</li>
<li>What else can you measure to help you intepret the solution?</li>
</ol>
<blockquote>
<p>Principal Components and Common Factor Anlayses have a potentially pesky drawback: Whereas distance and (dis)similarities are more “direct” measures (e.g., asking participants to rate similarity doesn’t impose any particular psychological dimension on their judgments), likert-based questions may only indirectly get at which dimensions participants actually use to make judgments; these kinds of questions assume participants use the dimensions provided by the researcher. Keep this in mind as I walk through Principal Components and Common Factor Anlayses below.</p>
</blockquote>
</div>
<div id="measuring-variables-we-cant-observe" class="section level1">
<h1>Measuring variables we can’t observe</h1>
<blockquote>
<p>If we think psychological constructs like extraversion, social dominance orientation, or depression exist, then we should be able to measure them. We can’t measure these constructs directly like we can measure blood pressure or light absorption. But we can ask people questions that we believe index parts of what we mean by, say, extroversion (e.g., outgoing, sociable, acts like a leader). The questions we ask make up our measurement “test” (e.g., like a test of extroversion), and the scores on our test comprise two parts: truth and error (random or systematic). In other words, unobservable psychological constructs and error can explain the scores we can observe from the tests we administer. This is the gist of classical test theory.</p>
</blockquote>
</div>
<div id="partitioning-variance" class="section level1">
<h1>Partitioning variance</h1>
<blockquote>
<p>A factor analysis models the variability in item responses (e.g., responses to questions on a test). Some of that variability can be explained by the relationship between items; some can be explained by what is “special” about the items; and some can be explained by error, random or systematic.</p>
</blockquote>
<div id="common-variance" class="section level2">
<h2>Common Variance</h2>
<blockquote>
<p>Common variance refers to variability shared among the items (i.e., what can be explained by inter-item correlations)</p>
</blockquote>
<div id="communality" class="section level3">
<h3>Communality</h3>
<blockquote>
<p>Communality ( <em>h^2</em> ) is one definition of common variance and can take values between 0 and 1: <em>h^2</em> = means no observed variability can be explained, and 1 means all observed variability can be explained.</p>
</blockquote>
</div>
</div>
<div id="unique-variance-u2" class="section level2">
<h2>Unique Variance ( <em>u^2</em> )</h2>
<blockquote>
<p>Unique variance refers to not-common variance. It has two parts: specific and error.</p>
</blockquote>
<div id="specific-variance" class="section level3">
<h3>Specific Variance</h3>
<blockquote>
<p>Specific variance refers to variance explained by “special” parts of particular items. For example, one of the extraversion items from the HEXACO asks whether respondents agree with the item, “Am usually active and full of energy.” It’s possible some of the variability in agreement to this item comes from people who conider themselves active because they exercise a lot or active because they’re talking to people a lot (i.e., physically active vs. social active).</p>
</blockquote>
</div>
<div id="error-variance" class="section level3">
<h3>Error Variance</h3>
<blockquote>
<p>Error variance refers to any variability that can’t be explained by common or specific variance. For example, people might respond different because they just received a text message. Note that it’s difficult to distinguish specific variance from error variance unless people take the same test at least 2 times. This would allow you to look for response patterns that emerge across testing contexts that can’t be explained by common variance.</p>
</blockquote>
</div>
</div>
</div>
<div id="whats-the-difference-between-principal-components-analysis-and-common-factor-analysis" class="section level1">
<h1>What’s the difference between Principal Components Analysis and Common Factor Analysis?</h1>
<blockquote>
<p>The major difference in these techniques boils down to error: Whereas the Principal Components model assumes that all variance is common variance, the Common Factor model partitions variance into common and unique variance (which includes error). Another way to think about the difference is that the Principal Components model tries to explain as much variance as possible with a small number of components, and Common Factor model tries to explain the shared relationships among the items with a small number of factors after setting aside what is not shared (i.e., the item-specific parts and the error parts). Below, I demonstrate the differences in how these two techniques account for variance in a sample of Big Five Inventory questions.</p>
</blockquote>
</div>
<div id="big-five-inventory" class="section level1">
<h1>Big Five Inventory</h1>
<blockquote>
<p>help(bfi) and help(bfi.dictionary)<br />
“The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness.”</p>
</blockquote>
<blockquote>
<p>“The item data were collected using a 6 point response scale: 1 Very Inaccurate 2 Moderately Inaccurate 3 Slightly Inaccurate 4 Slightly Accurate 5 Moderately Accurate 6 Very Accurate”</p>
</blockquote>
<div id="sample-questions" class="section level2">
<h2>Sample Questions</h2>
<ol style="list-style-type: decimal">
<li>Am indifferent to the feelings of others.</li>
<li>Inquire about others’ well-being.</li>
<li>Know how to comfort others.</li>
<li>Love children.</li>
<li>Make people feel at ease.</li>
<li>Am exacting in my work.</li>
<li>Continue until everything is perfect.</li>
<li>Do things according to a plan.</li>
<li>Do things in a half-way manner.</li>
<li>Waste my time.</li>
<li>Don’t talk a lot.</li>
<li>Find it difficult to approach others.</li>
<li>Know how to captivate people.</li>
<li>Make friends easily.</li>
<li>Take charge.</li>
<li>Get angry easily.</li>
<li>Get irritated easily.</li>
<li>Have frequent mood swings.</li>
<li>Often feel blue.</li>
<li>Panic easily.</li>
<li>Am full of ideas.</li>
<li>Avoid difficult reading material.</li>
<li>Carry the conversation to a higher level.</li>
<li>Spend time reflecting on things.</li>
<li>Will not probe deeply into a subject.</li>
</ol>
<div id="correlation-matrix" class="section level3">
<h3>Correlation matrix</h3>
<pre class="r"><code>bfi %&gt;%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %&gt;% 
  cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% 
  round(2) %&gt;% 
  as.data.frame() %&gt;% 
  mutate(item1 = rownames(.)) %&gt;% 
  gather(key = item2, value = r, -item1) %&gt;% 
  ggplot(mapping = aes(x = item1, y = item2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-9-1.png" width="1512" /></p>
<div id="interpretation-2" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>Darker purple tiles index more positive correlations, and darker red tiles index more negative correlations. Notice that 5 x 5 boxes of darker colors (i.e., more purple or more red, less white) emerge among similarly labeled items (i.e., the A items correlate more with A items than with C items or with O items)</p>
</blockquote>
</div>
</div>
<div id="histograms" class="section level3">
<h3>Histograms</h3>
<pre class="r"><code>bfi %&gt;%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %&gt;% 
  gather(key = variable, value = response) %&gt;% 
  ggplot(mapping = aes(x = response)) +
  geom_histogram(binwidth = 1, color = &quot;white&quot;) +
  facet_wrap(facets = ~ variable)</code></pre>
<pre><code>## Warning: Removed 508 rows containing non-finite values (stat_bin).</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div id="interpretation-3" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>Each plot represents a histogram for a specific item. Bigger bars mean more participants gave that response. Notice that items vary in their response distributions (e.g., some items receive higher agreement than others, and some items receive more spread out agreement responses)</p>
</blockquote>
</div>
</div>
<div id="parallel-analysis" class="section level3">
<h3>Parallel Analysis</h3>
<blockquote>
<p>help(fa.parallel)<br />
“One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree” plot of the successive eigenvalues. Sharp breaks in the plot suggest the appropriate number of components or factors to extract. “Parallel” analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original.”</p>
</blockquote>
<pre class="r"><code>bfi %&gt;%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %&gt;% 
  fa.parallel(fm = &quot;minres&quot;, fa = &quot;both&quot;)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre><code>## Parallel analysis suggests that the number of factors =  6  and the number of components =  6</code></pre>
<div id="interpretation-4" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>As in the example from Multidimensional Scaling, one can artfully select how many dimensions to extract based on when the lines in these plots stop falling so precipitously. As an added benefit of parallel analysis, you can use the red lines to compare the eigenvalues to what you would expect from random data generated from the same number of items. William Revelle the package developer suggests using the plot of triangles (the common factor based eigenvalues). The output tells you how many factors you could extract based on where the blue lines cross the red dotted lines.</p>
</blockquote>
</div>
</div>
</div>
</div>
<div id="principal-components-analysis" class="section level1">
<h1>Principal Components Analysis</h1>
<div id="save-only-the-25-items-from-the-test-ignore-reverse-worded-items" class="section level2">
<h2>Save only the 25 items from the test (ignore reverse-worded items)</h2>
<pre class="r"><code>bfi25 &lt;- bfi %&gt;%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5)</code></pre>
</div>
<div id="extract-5-components-even-though-the-parallel-analysis-suggested-6" class="section level2">
<h2>Extract 5 components even though the parallel analysis suggested 6</h2>
<pre class="r"><code>principal(bfi25, nfactors = 5, rotate = &quot;varimax&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Principal Components Analysis
## Call: principal(r = bfi25, nfactors = 5, rotate = &quot;varimax&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##    item   RC2   RC1   RC3   RC5   RC4   h2   u2 com
## N1   16  0.79  0.07 -0.04 -0.21 -0.08 0.69 0.31 1.2
## N3   18  0.79 -0.04 -0.06 -0.03  0.00 0.63 0.37 1.0
## N2   17  0.79  0.04 -0.03 -0.19 -0.02 0.66 0.34 1.1
## N4   19  0.65 -0.34 -0.17  0.02  0.09 0.57 0.43 1.7
## N5   20  0.63 -0.16 -0.01  0.15 -0.17 0.48 0.52 1.4
## E2   12  0.27 -0.72 -0.07 -0.08 -0.02 0.60 0.40 1.3
## E4   14 -0.10  0.70  0.09  0.28 -0.12 0.61 0.39 1.5
## E1   11  0.04 -0.68  0.09 -0.07 -0.05 0.48 0.52 1.1
## E3   13  0.04  0.63  0.07  0.24  0.26 0.53 0.47 1.7
## E5   15  0.05  0.58  0.34  0.04  0.20 0.50 0.50 1.9
## C2    7  0.11  0.04  0.74  0.10  0.09 0.58 0.42 1.1
## C3    8 -0.01  0.00  0.67  0.13 -0.04 0.47 0.53 1.1
## C4    9  0.28 -0.04 -0.67 -0.05 -0.12 0.54 0.46 1.4
## C1    6  0.03  0.07  0.65 -0.01  0.21 0.47 0.53 1.2
## C5   10  0.33 -0.17 -0.62 -0.05  0.06 0.52 0.48 1.7
## A2    2  0.03  0.21  0.13  0.71  0.07 0.57 0.43 1.3
## A3    3  0.01  0.35  0.09  0.68  0.05 0.59 0.41 1.6
## A1    1  0.16  0.13  0.08 -0.63 -0.13 0.46 0.54 1.4
## A5    5 -0.12  0.44  0.08  0.56  0.04 0.54 0.46 2.1
## A4    4 -0.06  0.19  0.26  0.53 -0.17 0.41 0.59 2.0
## O5   25  0.12  0.01 -0.04 -0.03 -0.68 0.48 0.52 1.1
## O3   23  0.04  0.36  0.08  0.11  0.64 0.55 0.45 1.7
## O2   22  0.23  0.03 -0.09  0.11 -0.59 0.43 0.57 1.4
## O1   21  0.02  0.28  0.13  0.02  0.59 0.44 0.56 1.5
## O4   24  0.28 -0.24 -0.02  0.24  0.50 0.44 0.56 2.6
## 
##                        RC2  RC1  RC3  RC5  RC4
## SS loadings           3.18 3.09 2.56 2.32 2.11
## Proportion Var        0.13 0.12 0.10 0.09 0.08
## Cumulative Var        0.13 0.25 0.35 0.45 0.53
## Proportion Explained  0.24 0.23 0.19 0.17 0.16
## Cumulative Proportion 0.24 0.47 0.67 0.84 1.00
## 
## Mean item complexity =  1.5
## Test of the hypothesis that 5 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.06 
##  with the empirical chi square  5469.42  with prob &lt;  0 
## 
## Fit based upon off diagonal values = 0.92</code></pre>
<pre class="r"><code># plot
principal(bfi25, nfactors = 5, rotate = &quot;varimax&quot;) %&gt;% 
  fa.diagram(sort = TRUE, errors = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div id="interpretation-5" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>The component diagram gives you a big picture sense of the solution: there are 5 components, and the items vary in how much they load (correlate) with each component. The output is a little harder to parse, but notice that items also correlate with other components. For more information about the output, see help(principal).</p>
</blockquote>
</div>
</div>
</div>
<div id="common-factor-analysis" class="section level1">
<h1>Common Factor Analysis</h1>
<div id="extract-5-factors-even-though-the-parallel-analysis-suggested-6" class="section level2">
<h2>Extract 5 factors even though the parallel analysis suggested 6</h2>
<pre class="r"><code>fa(bfi25, nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;minres&quot;) %&gt;% 
  print(sort = TRUE)</code></pre>
<pre><code>## Loading required namespace: GPArotation</code></pre>
<pre><code>## Factor Analysis using method =  minres
## Call: fa(r = bfi25, nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;minres&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##    item   MR2   MR1   MR3   MR5   MR4   h2   u2 com
## N1   16  0.81  0.10  0.00 -0.11 -0.05 0.65 0.35 1.1
## N2   17  0.78  0.04  0.01 -0.09  0.01 0.60 0.40 1.0
## N3   18  0.71 -0.10 -0.04  0.08  0.02 0.55 0.45 1.1
## N5   20  0.49 -0.20  0.00  0.21 -0.15 0.35 0.65 2.0
## N4   19  0.47 -0.39 -0.14  0.09  0.08 0.49 0.51 2.3
## E2   12  0.10 -0.68 -0.02 -0.05 -0.06 0.54 0.46 1.1
## E4   14  0.01  0.59  0.02  0.29 -0.08 0.53 0.47 1.5
## E1   11 -0.06 -0.56  0.11 -0.08 -0.10 0.35 0.65 1.2
## E5   15  0.15  0.42  0.27  0.05  0.21 0.40 0.60 2.6
## E3   13  0.08  0.42  0.00  0.25  0.28 0.44 0.56 2.6
## C2    7  0.15 -0.09  0.67  0.08  0.04 0.45 0.55 1.2
## C4    9  0.17  0.00 -0.61  0.04 -0.05 0.45 0.55 1.2
## C3    8  0.03 -0.06  0.57  0.09 -0.07 0.32 0.68 1.1
## C5   10  0.19 -0.14 -0.55  0.02  0.09 0.43 0.57 1.4
## C1    6  0.07 -0.03  0.55 -0.02  0.15 0.33 0.67 1.2
## A3    3 -0.03  0.12  0.02  0.66  0.03 0.52 0.48 1.1
## A2    2 -0.02  0.00  0.08  0.64  0.03 0.45 0.55 1.0
## A5    5 -0.11  0.23  0.01  0.53  0.04 0.46 0.54 1.5
## A4    4 -0.06  0.06  0.19  0.43 -0.15 0.28 0.72 1.7
## A1    1  0.21  0.17  0.07 -0.41 -0.06 0.19 0.81 2.0
## O3   23  0.03  0.15  0.02  0.08  0.61 0.46 0.54 1.2
## O5   25  0.13  0.10 -0.03  0.04 -0.54 0.30 0.70 1.2
## O1   21  0.02  0.10  0.07  0.02  0.51 0.31 0.69 1.1
## O2   22  0.19  0.06 -0.08  0.16 -0.46 0.26 0.74 1.7
## O4   24  0.13 -0.32 -0.02  0.17  0.37 0.25 0.75 2.7
## 
##                        MR2  MR1  MR3  MR5  MR4
## SS loadings           2.57 2.20 2.03 1.99 1.59
## Proportion Var        0.10 0.09 0.08 0.08 0.06
## Cumulative Var        0.10 0.19 0.27 0.35 0.41
## Proportion Explained  0.25 0.21 0.20 0.19 0.15
## Cumulative Proportion 0.25 0.46 0.66 0.85 1.00
## 
##  With factor correlations of 
##       MR2   MR1   MR3   MR5   MR4
## MR2  1.00 -0.21 -0.19 -0.04 -0.01
## MR1 -0.21  1.00  0.23  0.33  0.17
## MR3 -0.19  0.23  1.00  0.20  0.19
## MR5 -0.04  0.33  0.20  1.00  0.19
## MR4 -0.01  0.17  0.19  0.19  1.00
## 
## Mean item complexity =  1.5
## Test of the hypothesis that 5 factors are sufficient.
## 
## The degrees of freedom for the null model are  300  and the objective function was  7.23 with Chi Square of  20163.79
## The degrees of freedom for the model are 185  and the objective function was  0.65 
## 
## The root mean square of the residuals (RMSR) is  0.03 
## The df corrected root mean square of the residuals is  0.04 
## 
## The harmonic number of observations is  2762 with the empirical chi square  1392.16  with prob &lt;  5.6e-184 
## The total number of observations was  2800  with Likelihood Chi Square =  1808.94  with prob &lt;  4.3e-264 
## 
## Tucker Lewis Index of factoring reliability =  0.867
## RMSEA index =  0.056  and the 90 % confidence intervals are  0.054 0.058
## BIC =  340.53
## Fit based upon off diagonal values = 0.98
## Measures of factor score adequacy             
##                                                    MR2  MR1  MR3  MR5  MR4
## Correlation of (regression) scores with factors   0.92 0.89 0.88 0.88 0.84
## Multiple R square of scores with factors          0.85 0.79 0.77 0.77 0.71
## Minimum correlation of possible factor scores     0.70 0.59 0.54 0.54 0.42</code></pre>
<pre class="r"><code># plot
fa(bfi25, nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;minres&quot;) %&gt;% 
  fa.diagram(sort = TRUE, errors = TRUE)</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<div id="interpretation-6" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>The factor diagram gives you a big picture sense of the solution: there are 5 factors, and the items vary in how much they load (correlate) with each factor. The output is a little harder to parse, but notice that items also correlate with other factors. Factors also correlate with other factors. You also get some model fit indices, which give you a sense of how much the model impled correlation matrix deviates from the original matrix (like residuals in regression). For more information about the output, see help(fa).</p>
</blockquote>
</div>
</div>
</div>
<div id="compare-correlations-among-factors-extracted-from-each-model" class="section level1">
<h1>Compare correlations among factors extracted from each model</h1>
<div id="refit-models-to-make-later-code-easier-to-read-i.e.-so-you-dont-have-to-read-code-for-one-function-nested-inside-another-function" class="section level2">
<h2>Refit models to make later code easier to read (i.e., so you don’t have to read code for one function nested inside another function)</h2>
<pre class="r"><code># fit principal componenets analysis
principal.fit1 &lt;- principal(bfi25, nfactors = 5, rotate = &quot;varimax&quot;)

# fit factor analysis
fa.fit1 &lt;- fa(bfi25, nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;minres&quot;)</code></pre>
</div>
<div id="factor-congruence" class="section level2">
<h2>factor congruence</h2>
<blockquote>
<p>help(factor.congruence)<br />
“Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1948), this is frequently known as the Tucker index of factor congruence.”</p>
</blockquote>
<pre class="r"><code>factor.congruence(list(principal.fit1, fa.fit1)) %&gt;% 
  as.data.frame() %&gt;% 
  mutate(factor1 = rownames(.)) %&gt;% 
  gather(key = factor2, value = r, -factor1) %&gt;% 
  ggplot(mapping = aes(x = factor1, y = factor2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2()</code></pre>
<p><img src="/post/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology/2019-02-14-using-principal-components-or-common-factor-analysis-in-social-psychology_files/figure-html/unnamed-chunk-16-1.png" width="1512" /></p>
<div id="interpretation-7" class="section level4">
<h4>Interpretation</h4>
<blockquote>
<p>MR variables refer to factors from the common factor model, and RC variable refer to components from the components model. Darker blue tiles suggest two factors are more congruent, and whiter and darker red tiles suggest two factors are less congruent. These give you a sense of how different the loadings are depending on the model you use. In this case, we used one model that assumes all variance is common and the factors are uncorrelated (Component Analysis) and another model that assumes some variance is common but some is also special or due to error, and factors are correlated (Common Factor Analysis).</p>
</blockquote>
</div>
</div>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<blockquote>
<ul>
<li>Gonzalez, R. (February, 2016). <em>Lecture Notes #10: MDS &amp; Tree Structures.</em> Retrieved from <a href="http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf" class="uri">http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf</a> on February 17, 2019.<br />
</li>
<li>Revelle, W. (n.d.). <em>An introduction to psychometric theory with applications in R.</em> Retreived from <a href="http://www.personality-project.org/r/book/" class="uri">http://www.personality-project.org/r/book/</a> on February 17, 2019.</li>
<li>A practical introduction to factor analysis: exploratory factor analysis. <em>UCLA: Statistical Consulting Group.</em> Retreived from <a href="https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/" class="uri">https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/</a> on February 17, 2019.</li>
</ul>
</blockquote>
</div>
<div id="general-word-of-caution" class="section level1">
<h1>General word of caution</h1>
<blockquote>
<p>Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don’t assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.</p>
</blockquote>
</div>
