---
title: Using Principal Components or Common Factor Analysis in Social Psychology
author: Nick Michalak
date: '2019-02-14'
slug: using-principal-components-or-common-factor-analysis-in-social-psychology
categories:
  - common factor analysis
  - tutorial
  - principal components analysis
  - multidimensional scaling
  - R
  - measurement
  - demonstration
  - reliability
tags:
  - common factor analysis
  - tutorial
  - principal component analysis
  - multidimensional scaling
  - R
  - measurement
  - reliability
  - demonstration
image:
  caption: ''
  focal_point: ''
---

# Multidimensional Scaling, the precursor to Principal Components Analysis, Common Factor Analysis, and related techniques
> Multidimensional scaling is an exploratory technique that uses distances or disimilarities between objects to create a multidimensional representation of those objects in metric space. In other words, multidimensional scaling uses data about the distance (e.g., miles between cities) or disimilarity (e.g., how (dis)similar are apples and tomatoes?) among a set of objects to "search" for some metric space that represents those objects and their relations to each other. Metric space is a fancy term for a set of objects and some metric that satisfy a list of axioms.  

> Here is one way to represent the distance (d) axioms:

1. $$d_{ij} > d_{ii} = 0 \text{,  for } i \neq j.$$
2. $$d_{ij} = d_{ji}.$$
3. $$d_{ij} \leq d_{ik} + d_{kj}.$$  

> Here is one way to represent the similarity (s) axioms:  

1. $$s_{ii} > s_{ij}.$$
2. $$s_{ij} = s_ji.$$
3. $$\text{a large } s_{ij} \text{ implies } d_{ik} \approx d_{kj}.$$  

> If you're familiar with the Pythagorean Theorem $$c = \sqrt{a^2 + b^2}$$, and you have a sense of what "similar" means, then these axioms should be intutuive to you. With regard to Multidimensional Scaling, the big idea here is that the data and the axioms constrain the solution you get. For example, if you're making a map of U.S. cities—and you want an accurate map of the cities—then including 100,000 cities in your analysis will place more constraints on your solution than including 10 cities. More contraints in this case will give you a better picture of 10 cities and their relation to each other (e.g., New York is north of Miami) than fewer constraints.  

> This should be more clear in the example below.

## Install packages and/or load libraries
> I'll use these packages throughout this post.

```{r, warning = FALSE, message = FALSE}

# install.packages("tidyverse")
# install.packages("knitr")
# install.packages("haven")
# install.packages("maps")
# install.packages("psych")

library(tidyverse)
library(knitr)
library(haven)
library(maps)
library(psych)

# use select from dplyr
select <- dplyr::select

```

## Data
> `help("UScitiesD")`  
> "UScitiesD gives “straight line” distances between 10 cities in the US."

```{r}

UScitiesD %>% 
  as.matrix() %>% 
  kable()

```

### Represent the distance matrix with colors
> 1. Convert the distance object into a `data.frame`.  
> 2. Restructure the `data.frame` so each distance value gets its own row, and each distance value corresponds to two city names (even cities paired with themselves, so distance = 0).  
> 3. Plot the distance values on tiles that are colored by the size of the distance.

```{r, fig.width = 10.5, fig.height = 7.5}

UScitiesD %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  mutate(city1 = rownames(.)) %>% 
  gather(key = city2, value = distance, -city1) %>% 
  ggplot(mapping = aes(x = city1, y = city2, fill = distance, label = distance)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

#### Interpretation
> Darker tiles index that two cities are relatively far apart, and lighter tiles index that two cities are relatively close together (e.g., white tiles = a city is right next to itself)

### Metric Multidimensional Scaling (2-dimensions, like a map)
> `help(cmdscale)`  
> "Classical multidimensional scaling (MDS) of a data matrix. Also known as principal coordinates analysis (Gower, 1966)."

```{r}

# give it distance values and number of dimensions
# also ask for the function to output eigenvalues
cmdscale.fit1 <- cmdscale(UScitiesD, k = 2, eig = TRUE)

```

### Plot eigenvalues sorted by size (i.e., screeplot)
> You can use eigenvalues to give you a sense of how many dimensions can capture most of the information from original data. A "big" eigenvalue means a lot of information is contained in a given dimension. You can tell how big the eigenvalues are by looking a a plot of the digenvalues sorted by size. Analysts typically determine the number of dimensions to use by counting eigenvalues from left to right until the pattern of points flattens out (i.e., there is little leftover information contained in the remaining dimensions).

```{r}

tibble(index = 1:length(cmdscale.fit1$eig),
       eigenvalue = cmdscale.fit1$eig) %>% 
  ggplot(mapping = aes(x = index, y = eigenvalue)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::comma)

```

#### Interpretation
> Because the lines stop dropping after 2-3 points, this suggest most of the information can be "captured" by 2-3 dimensions. To make a plot easier to interpret, I already asked for 2 dimensions.

### Store points from the fitted configuration/solution in a tibble `data.frame`
> The first and second dimension are stored as columns in the `points` object from the `cmdscale` function

```{r}

cmdscale.data <- tibble(city = rownames(cmdscale.fit1$points),
                        dimension1 = cmdscale.fit1$points[, 1],
                        dimension2 = cmdscale.fit1$points[, 2])

```

#### Plot the solution
> Note that I asked ggplot2 to plot the negative dimension scores to make the plot more interpretable compared to a map of the U.S. (i.e., real life)

```{r}

cmdscale.data %>% 
  ggplot(mapping = aes(x = -dimension1, y = -dimension2, label = city)) +
  geom_point() +
  geom_label(nudge_y = 100)

```

#### Compare the solution above to a map of the U.S. below
> Because we only used pairwise distances among ten U.S. cities, the solution imperfectly represented real relations among these cities. For example, Miami seems to be in the right spot on the "Southeastern quadrant" of the plot, it seems to sit on almost a horizontal line with Houston; in real life, Houston is northwest of Miami, not directly west like the solution implies.

```{r}

map_data("state") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()

```

# Principal Components and Common Factor Anlayses are special cases of Multidimensional Scaling
> The major difference between Multidimensional Scaling and Principal Components and Common Factor Anlayses is that Multidimensional Scaling uses distances and (dis)similarity matrices and Principal Components and Common Factor Anlayses use covariance and correlation  matrices. But all these techniques share common issues:

1. How do you collect data (e.g., (dis)simlarities between items, likert responses to a question)  
2. How do you rotate the solution?
3. What else can you measure to help you intepret the solution?  

> Principal Components and Common Factor Anlayses have a potentially pesky drawback: Whereas distance and (dis)similarities are more "direct" measures (e.g., asking participants to rate similarity doesn't impose any particular psychological dimension on their judgments), likert-based questions may only indirectly get at which dimensions participants actually use to make judgments; these kinds of questions assume participants use the dimensions provided by the researcher. Keep this in mind as I walk through Principal Components and Common Factor Anlayses below.

# Measuring variables we can't observe
> If we think psychological constructs like extraversion, social dominance orientation, or depression exist, then we should be able to measure them. We can't measure these constructs directly like we can measure blood pressure or light absorption. But we can ask people questions that we believe index parts of what we mean by, say, extroversion (e.g., outgoing, sociable, acts like a leader). The questions we ask make up our measurement "test" (e.g., like a test of extroversion), and the scores on our test comprise two parts: truth and error (random or systematic). In other words, unobservable psychological constructs and error can explain the scores we can observe from the tests we administer. This is the gist of classical test theory.

# Partitioning variance
> A factor analysis models the variability in item responses (e.g., responses to questions on a test). Some of that variability can be explained by the relationship between items; some can be explained by what is "special" about the items; and some can be explained by error, random or systematic.

## Common Variance
> Common variance refers to variability shared among the items (i.e., what can be explained by inter-item correlations)

### Communality
> Communality (*h^2^*) is one definition of common variance and can take values between 0 and 1: *h^2^* = 0 means no observed variability can be explained, and 1 means all observed variability can be explained.

## Unique Variance (*u^2^*)
> Unique variance refers to not-common variance. It has two parts: specific and error.

### Specific Variance
> Specific variance refers to variance explained by "special" parts of particular items. For example, one of the extraversion items from the [HEXACO](http://www.hexaco.org/) asks whether respondents agree with the item, "Am usually active and full of energy." It's possible some of the variability in agreement to this item comes from people who conider themselves active because they exercise a lot or active because they're talking to people a lot (i.e., physically active vs. social active).

### Error Variance
> Error variance refers to any variability that can't be explained by common or specific variance. For example, people might respond differently because they just received a text message. Note that it's difficult to distinguish specific variance from error variance unless people take the same test at least 2 times. This would allow you to look for response patterns that emerge across testing contexts that can't be explained by what's common among the questions.

# What's the difference between Principal Components Analysis and Common Factor Analysis?
> The major difference in these techniques boils down to error: Whereas the Principal Components model assumes that all variance is common variance, the Common Factor model partitions variance into common and unique variance (which includes error). Another way to think about the difference is that the Principal Components model tries to explain as much variance as possible with a small number of components, and Common Factor model tries to explain the shared relationships among the items with a small number of factors after setting aside what is not shared (i.e., the item-specific parts and the error parts). Below, I demonstrate the differences in how these two techniques account for variance in a sample of Big Five Inventory questions.

# Big Five Inventory
> `help(bfi)` and `help(bfi.dictionary)`  
> "The first 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Opennness."  

> "The item data were collected using a 6 point response scale: 1 Very Inaccurate 2 Moderately Inaccurate 3 Slightly Inaccurate 4 Slightly Accurate 5 Moderately Accurate 6 Very Accurate"

## Sample Questions
1. Am indifferent to the feelings of others.
2. Inquire about others' well-being.
3. Know how to comfort others.
4. Love children.
5. Make people feel at ease.
6. Am exacting in my work.
7. Continue until everything is perfect.
8. Do things according to a plan.
9. Do things in a half-way manner.
10. Waste my time.
11. Don't talk a lot.
12. Find it difficult to approach others.
13. Know how to captivate people.
14. Make friends easily.
15. Take charge.
16. Get angry easily.
17. Get irritated easily.
18. Have frequent mood swings.
19. Often feel blue.
20. Panic easily.
21. Am full of ideas.
22. Avoid difficult reading material.
23. Carry the conversation to a higher level.
24. Spend time reflecting on things.
25. Will not probe deeply into a subject.

### Correlation matrix

```{r, fig.width = 15.75, fig.height = 11.25}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  round(2) %>% 
  as.data.frame() %>% 
  mutate(item1 = rownames(.)) %>% 
  gather(key = item2, value = r, -item1) %>% 
  ggplot(mapping = aes(x = item1, y = item2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

#### Interpretation
> Darker purple tiles index more positive correlations, and darker red tiles index more negative correlations. Notice that 5 x 5 boxes of darker colors (i.e., more purple or more red, less white) emerge among similarly labeled items (i.e., the A items correlate more with A items than with C items or with O items)

### Histograms

```{r, fig.width = 10.5, fig.height = 7.5}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  gather(key = variable, value = response) %>% 
  ggplot(mapping = aes(x = response)) +
  geom_histogram(binwidth = 1, color = "white") +
  facet_wrap(facets = ~ variable)

```

#### Interpretation
> Each plot represents a histogram for a specific item. Bigger bars mean more participants gave that response. Notice that items vary in their response distributions (e.g., some items receive higher agreement than others, and some items receive more spread out agreement responses)

### Parallel Analysis
> `help(fa.parallel)`  
> "One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree" plot of the successive eigenvalues. Sharp breaks in the plot suggest the appropriate number of components or factors to extract. “Parallel" analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original."

```{r, fig.width = 10.5, fig.height = 7.5}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  fa.parallel(fm = "minres", fa = "both")

```

#### Interpretation
> As in the example from Multidimensional Scaling, one can artfully select how many dimensions to extract based on when the lines in these plots stop falling so precipitously. As an added benefit of parallel analysis, you can use the red lines to compare the eigenvalues to what you would expect from random data generated from the same number of items. William Revelle the package developer suggests using the plot of triangles (the common factor based eigenvalues). The output tells you how many factors you could extract based on where the blue lines cross the red dotted lines.

# Principal Components Analysis

## Save only the 25 items from the test (ignore reverse-worded items)

```{r}

bfi25 <- bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5)

```

## Extract 5 components even though the parallel analysis suggested 6

```{r, fig.width = 10.5, fig.height = 7.5}

principal(bfi25, nfactors = 5, rotate = "varimax") %>% 
  print(sort = TRUE)

# plot
principal(bfi25, nfactors = 5, rotate = "varimax") %>% 
  fa.diagram(sort = TRUE, errors = TRUE)

```

#### Interpretation
> The component diagram gives you a big picture sense of the solution: there are 5 components, and the items vary in how much they load (correlate) with each component. The output is a little harder to parse, but notice that items also correlate with other components. For more information about the output, see `help(principal)`.

# Common Factor Analysis

## Extract 5 factors even though the parallel analysis suggested 6

```{r, fig.width = 10.5, fig.height = 7.5}

fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres") %>% 
  print(sort = TRUE)

# plot
fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres") %>% 
  fa.diagram(sort = TRUE, errors = TRUE)

```

#### Interpretation
> The factor diagram gives you a big picture sense of the solution: there are 5 factors, and the items vary in how much they load (correlate) with each factor. The output is a little harder to parse, but notice that items also correlate with other factors. Factors also correlate with other factors. You also get some model fit indices, which give you a sense of how much the model impled correlation matrix deviates from the original matrix (like residuals in regression). For more information about the output, see `help(fa)`.

# Compare correlations among factors extracted from each model

## Refit models to make later code easier to read (i.e., so you don't have to read code for one function nested inside another function)

```{r}

# fit principal componenets analysis
principal.fit1 <- principal(bfi25, nfactors = 5, rotate = "varimax")

# fit factor analysis
fa.fit1 <- fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres")

```

## Are the factors and components congruent?
> `help(factor.congruence)`  
> "Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1948), this is frequently known as the Tucker index of factor congruence."

```{r, fig.width = 15.75, fig.height = 11.25}

factor.congruence(list(principal.fit1, fa.fit1)) %>% 
  as.data.frame() %>% 
  mutate(factor1 = rownames(.)) %>% 
  gather(key = factor2, value = r, -factor1) %>% 
  ggplot(mapping = aes(x = factor1, y = factor2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2()

```

#### Interpretation
> MR variables refer to factors from the common factor model, and RC variable refer to components from the  components model. Darker blue tiles suggest two factors are more congruent, and whiter and darker red tiles suggest two "factors"^1^ are less congruent. These give you a sense of how different the loadings are depending on the model you use. In this case, we used one model that assumes all variance is common and the factors are uncorrelated (Component Analysis) and another model that assumes some variance is common but some is also special or due to error, and factors are correlated (Common Factor Analysis).

# Footnotes
1. Although it many cases it's useful to distinguish between factors and components (I do in this post), the information you need to interpret differences in solutions boils down to differences among models, not terms. Here's a case where it's confusing to refer to factors and components because it makes it seems like the congruence test is comparing apples to oranges; but it's comparing "factor" (or "component") solutions from two different models.

# Resources
> * Gonzalez, R. (February, 2016). *Lecture Notes #10: MDS & Tree Structures.* Retrieved from [http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf](http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf) on February 17, 2019.  
> * Revelle, W. (n.d.). *An introduction to psychometric theory with applications in R.* Retreived from [http://www.personality-project.org/r/book/](http://www.personality-project.org/r/book/) on February 17, 2019.
> * A practical introduction to factor analysis: exploratory factor analysis. *UCLA: Statistical Consulting Group.* Retreived from [https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/](https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) on February 17, 2019.

# General word of caution
> Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don't assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.
