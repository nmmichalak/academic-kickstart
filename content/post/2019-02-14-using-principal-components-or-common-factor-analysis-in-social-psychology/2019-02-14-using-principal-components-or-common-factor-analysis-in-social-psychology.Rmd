---
title: Using Principal Components or Common Factor Analysis in Social Psychology
author: Nick Michalak
date: '2019-02-14'
slug: using-principal-components-or-common-factor-analysis-in-social-psychology
categories:
  - common factor analysis
  - tutorial
  - principal components analysis
  - multidimensional scaling
  - R
  - measurement
  - demonstration
  - reliability
tags:
  - common factor analysis
  - tutorial
  - principal component analysis
  - multidimensional scaling
  - R
  - measurement
  - reliability
  - demonstration
image:
  caption: ''
  focal_point: ''
---

Note. I'm not done with this post but it's fun to build a new website and see your posts appear so pretty online ... so here it is!

# Multidimensional Scaling
> Multidimensional scaling is an exploratory technique that uses distances or disimilarities between objects to create a multidimensional representation of those objects in metric space. In other words, multidimensional scaling uses data about the distance (e.g., miles between cities) or disimilarity (e.g., how (dis)similar are apples and tomatoes?) among a set of objects to "search" for some metric space that represents those objects and their relations to each other. Metric space is a fancy term for a set of objects and some metric that satisfy a list of axioms.  

> Here is one way to represent the distance (d) axioms:

1. $$d_{ij} > d_{ii} = 0 \text{,  for } i \neq j.$$
2. $$d_{ij} = d_{ji}.$$
3. $$d_{ij} \leq d_{ik} + d_{kj}.$$  

> Here is one way to represent the similarity (s) axioms:  

1. $$s_{ii} > s_{ij}.$$
2. $$s_{ij} = s_ji.$$
3. $$\text{a large } s_{ij} \text{ implies } d_{ik} \approx d_{kj}.$$  

> The idea here is that the data and the axioms constrain the solution you get. For example, if you're making a map of U.S. cities—and you want a accurate map of the cities—then including 100,000 cities in your analysis will place more constraints on your solution than including 10 cities. More contraints in this case will give you a better picture of 10 cities and their relation to each other (e.g., New York is north of Miami) than fewer constraints.  
> This should be more clear in the example below.

## Install packages and/or load libraries
> I'll use these packages throughout this post.

```{r, warning = FALSE, message = FALSE}

# install.packages("tidyverse")
# install.packages("knitr")
# install.packages("haven")
# install.packages("maps")
# install.packages("psych")

library(tidyverse)
library(knitr)
library(haven)
library(maps)
library(psych)

# use select from dplyr
select <- dplyr::select

```

## Data
> `help("UScitiesD")`  
> "UScitiesD gives “straight line” distances between 10 cities in the US."

```{r}

UScitiesD %>% 
  as.matrix() %>% 
  kable()

```

### Represent the distance matrix with colors
> 1. Convert the distance object into a `data.frame`.  
> 2. Restructure the `data.frame` so each distance value gets its own row, and each distance value corresponds to two city names (even cities paired with themselves, so distance = 0).  
> 3. Plot the distance values on tiles that are colored by the size of the distance.

```{r, fig.width = 10.5, fig.height = 7.5}

UScitiesD %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  mutate(city1 = rownames(.)) %>% 
  gather(key = city2, value = distance, -city1) %>% 
  ggplot(mapping = aes(x = city1, y = city2, fill = distance, label = distance)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

### Metric Multidimensional Scaling (2-dimensions, like a map)
> `help("cmdscale")`  
> "Classical multidimensional scaling (MDS) of a data matrix. Also known as principal coordinates analysis (Gower, 1966)."

```{r}

# give it distance values and number of dimensions
# also ask for the function to output eigenvalues
cmdscale.fit1 <- cmdscale(UScitiesD, k = 2, eig = TRUE)

```

### Plot eigenvalues sorted by size (i.e., screeplot)
> You can use eigenvalues to give you a sense of how many dimensions can capture most of the information from original data. A "big" eigenvalue means a lot of information is contained in a given dimension. You can tell how big the eigenvalues are by looking a a plot of the digenvalues sorted by size. Analysts typically determine the number of dimensions to use by counting eigenvalues from left to right until the pattern of points flattens out (i.e., there is little leftover information contain in the remaining dimensions).

```{r}

tibble(index = 1:length(cmdscale.fit1$eig),
       eigenvalue = cmdscale.fit1$eig) %>% 
  ggplot(mapping = aes(x = index, y = eigenvalue)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::comma)

```

### Store points from the fitted configuration/solution in a tibble `data.frame`

```{r}

cmdscale.data <- tibble(city = rownames(cmdscale.fit1$points),
                        dimension1 = cmdscale.fit1$points[, 1],
                        dimension2 = cmdscale.fit1$points[, 2])

```

#### Plot the solution
> Note that I asked ggplot2 to plot the negative dimension scores to make the plot more interpretable compared to a map of the U.S. (i.e., real life)

```{r}

cmdscale.data %>% 
  ggplot(mapping = aes(x = -dimension1, y = -dimension2, label = city)) +
  geom_point() +
  geom_label(nudge_y = 100)

```

#### Compare the solution above to a map of the U.S. below
> Because we only used pairwise distances among ten U.S. cities, the solution imperfectly represented real relations among these cities. For example, Miami seems to be in the right spot on the "Southeastern quadrant" of the plot, but New York and Washington D.C. are placed in the "Southwestern quadrant" of the plot.

```{r}

map_data("state") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()

```

# Principal Components and Common Factor Anlayses are special cases of Multidimensional Scaling
> The major difference between Multidimensional Scaling and Principal Components and Common Factor Anlayses is that Multidimensional Scaling uses distances and (dis)similarity matrices and Principal Components and Common Factor Anlayses use covariance and correlation  matrices. But all these techniques share common issues:  
1. How do you collect data (e.g., (dis)simlarities between items, likert responses to a question)  
2. How do you rotate the solution?
3. What else can you measure to help you intepret the solution?  
> Principal Components and Common Factor Anlayses have a potentially pesky drawback: Whereas distance and (dis)similarities are more "direct" measures (e.g., asking participants to rate similarity doesn't impose any particular psychological dimension on their judgments), likert-based questions may only indirectly get at which dimensions participants actually use to make judgments; these kinds of questions assume participants use the dimensions provided by the researcher. Keep this in mind as I walk through Principal Components and Common Factor Anlayses below.

# Measuring variables we can't observe
> If we think psychological constructs like extraversion, social dominance orientation, or depression exist, then we should be able to measure them. We can't measure these constructs directly like we can measure blood pressure or light absorption. But we can ask people questions that we believe index parts of what we mean by, say, extroversion (e.g., outgoing, sociable, acts like a leader). The questions we ask make up our measurement "test" (e.g., like a test of extroversion), and the scores on our test comprise two parts: truth and error (random or systematic). In other words, unobservable psychological constructs and error can explain the scores we can observe from the tests we administer. This is classical test theory.

# Partitioning variance
> A factor analysis models the variability in item responses (e.g., responses to questions on a test). Some of that variability can be explained by the relationship between items; some can be explained by what is "special" about the items; and some can be explained by error, random or systematic.

## Common Variance
> Common variance refers to variability shared among the items (i.e., what can be explained by inter-item correlations)

### Communality
> Communality (_h^2_) is one definition of common variance and can take values between 0 and 1: _h^2_ = means no observed variability can be explained, and 1 means all observed variability can be explained.

## Unique Variance
> Unique variance refers to not-common variance. It has two parts: specific and error.

### Specific Variance
> Specific variance refers to variance explained by "special" parts of particular items. For example, one of the extraversion items from the HEXACO asks whether respondents agree with the item, "Am usually active and full of energy." It's possible some of the variability in agreement to this item comes from people who conider themselves active because they exercise a lot or active because they're talking to people a lot (i.e., physically active vs. social active).

### Error Variance
> Error variance refers to any variability that can't be explained by common or specific variance. For example, people might respond different because they just received a text message. Note that it's difficult to distinguish specific variance from error variance unless people take the same test at least 2 times. This would allow you to look for response patterns that emerge across testing contexts that can't be explained by common variance.

# Big Five Inventory

## Sample Questions
1. Am indifferent to the feelings of others.
2. Inquire about others' well-being.
3. Know how to comfort others.
4. Love children.
5. Make people feel at ease.
6. Am exacting in my work.
7. Continue until everything is perfect.
8. Do things according to a plan.
9. Do things in a half-way manner.
10. Waste my time.
11. Don't talk a lot.
12. Find it difficult to approach others.
13. Know how to captivate people.
14. Make friends easily.
15. Take charge.
16. Get angry easily.
17. Get irritated easily.
18. Have frequent mood swings.
19. Often feel blue.
20. Panic easily.
21. Am full of ideas.
22. Avoid difficult reading material.
23. Carry the conversation to a higher level.
24. Spend time reflecting on things.
25. Will not probe deeply into a subject.

### Correlation matrix

```{r, fig.width = 15.75, fig.height = 11.25}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  round(2) %>% 
  as.data.frame() %>% 
  mutate(item1 = rownames(.)) %>% 
  gather(key = item2, value = r, -item1) %>% 
  ggplot(mapping = aes(x = item1, y = item2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

### Histograms

```{r}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  gather(key = variable, value = response) %>% 
  ggplot(mapping = aes(x = response)) +
  geom_histogram(binwidth = 1, color = "white") +
  facet_wrap(facets = ~ variable)

```

### Parallel Analysis
> help(fa.parallel)
> "One way to determine the number of factors or components in a data matrix or a correlation matrix is to examine the “scree" plot of the successive eigenvalues. Sharp breaks in the plot suggest the appropriate number of components or factors to extract. “Parallel" analyis is an alternative technique that compares the scree of factors of the observed data with that of a random data matrix of the same size as the original."

```{r}

bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5) %>% 
  fa.parallel(fm = "minres", fa = "both")

```

# Principal Components Analysis

## Save only the 25 items from the test (ignore reverse-worded items)

```{r}

bfi25 <- bfi %>%
  select(A1, A2, A3, A4, A5, C1, C2, C3, C4, C5, E1, E2, E3, E4, E5, N1, N2, N3, N4, N5, O1, O2, O3, O4, O5)

```

## Extract 5 components even though the parallel analysis suggested 6

```{r}

principal(bfi25, nfactors = 5, rotate = "varimax") %>% 
  print(sort = TRUE)

# plot
principal(bfi25, nfactors = 5, rotate = "varimax") %>% 
  fa.diagram(sort = TRUE)

```

# Common Factor Analysis

## Extract 5 factors even though the parallel analysis suggested 6

```{r}

fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres") %>% 
  print(sort = TRUE)

# plot
fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres") %>% 
  fa.diagram(sort = TRUE)

```

# Compare correlations among factors extracted from each model

## Refit models to make later code easier to read (i.e., so you don't have to read code for one function nested inside another function)

```{r}

# fit principal componenets analysis
principal.fit1 <- principal(bfi25, nfactors = 5, rotate = "varimax")

# fit factor analysis
fa.fit1 <- fa(bfi25, nfactors = 5, rotate = "oblimin", fm = "minres")

```

## factor congruence
> help(factor.congruence)  
> "Given two sets of factor loadings, report their degree of congruence (vector cosine). Although first reported by Burt (1948), this is frequently known as the Tucker index of factor congruence."

```{r, fig.width = 15.75, fig.height = 11.25}

factor.congruence(list(principal.fit1, fa.fit1)) %>% 
  as.data.frame() %>% 
  mutate(factor1 = rownames(.)) %>% 
  gather(key = factor2, value = r, -factor1) %>% 
  ggplot(mapping = aes(x = factor1, y = factor2, fill = r, label = r)) +
  geom_tile() +
  geom_text() +
  scale_fill_gradient2()

```

# Resources
> * Gonzalez, R. (February, 2016). *Lecture Notes #10: MDS & Tree Structures.* Retrieved from [http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf](http://www-personal.umich.edu/~gonzo/coursenotes/file10.pdf) on February 17, 2019.  
> * Revelle, W. (n.d.). *An introduction to psychometric theory with applications in R.* Retreived from [http://www.personality-project.org/r/book/](http://www.personality-project.org/r/book/) on February 17, 2019.
> * A practical introduction to factor analysis: exploratory factor analysis. *UCLA: Statistical Consulting Group.* Retreived from [https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/](https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) on February 17, 2019.

# General word of caution
> Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don't assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.
